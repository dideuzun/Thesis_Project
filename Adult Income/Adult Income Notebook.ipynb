{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries and Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from a csv file\n",
    "df = pd.read_csv('CensusAdultIncome.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       47879 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      47876 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48568 non-null  object\n",
      " 14  income          48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- workclass: A categorical feature representing the type of income, such as private, self-employment, and government employment. Some missing values present.\n",
    "- fnlwgt: An integer feature with no description provided. No missing values.\n",
    "- education: A categorical feature representing the level of education \n",
    "- education-num: An integer feature representing the numerical encoding of **education** level.\n",
    "- occupation: A categorical feature representing the type of occupation, such as managerial, technical, and service-related occupations. Some missing values present.\n",
    "- native-country: A categorical feature representing the country of origin, including various countries such as the United States, Canada, and India. Some missing values present.\n",
    "- income: The target variable, a binary feature representing income level, with categories >50K and <=50K. No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cleaning the target variable and making it binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target value has values with dots, therefore 4 catagories\n",
    "df['income'] = df['income'].str.replace('.', '')\n",
    "\n",
    "# Replace the values in the target column with string '0' and '1'.\n",
    "df['income'] = df['income'].str.replace('<=50K', '0')  \n",
    "df['income'] = df['income'].str.replace('>50K', '1')   \n",
    "\n",
    "# Convert to integer\n",
    "df['income'] = df['income'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       47879 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      47876 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48568 non-null  object\n",
      " 14  income          48842 non-null  int32 \n",
      "dtypes: int32(1), int64(6), object(8)\n",
      "memory usage: 5.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print count and percentage of classes variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " 0    37155\n",
      "1    11687\n",
      "Name: income, dtype: int64 \n",
      "\n",
      "Percentage of each class: \n",
      " 0    76.071823\n",
      "1    23.928177\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Total number of rows:  48842\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts of unique values in the 'class' column \n",
    "class_counts = df['income'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each unique value in the 'class' column by dividing 'class_counts' by its sum and then multiplying by 100.\n",
    "class_percentages = class_counts / class_counts.sum() * 100\n",
    "\n",
    "print('Class counts:\\n' ,class_counts, '\\n')\n",
    "print('Percentage of each class: \\n' ,class_percentages)\n",
    "print('\\nTotal number of rows: ', df.shape[0])\n",
    "\n",
    "# Saving this for future use\n",
    "a = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Droping nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some values are like '?'. Replacing them as NaN \n",
    "df[df == '?'] = np.nan\n",
    "# Dropping the rows with NaN values in  'workclass', 'occupation', 'native-country' for the test dataset\n",
    "df.dropna(subset=['workclass', 'occupation', 'native-country'], inplace=True)\n",
    "df.dropna( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45222 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             45222 non-null  int64 \n",
      " 1   workclass       45222 non-null  object\n",
      " 2   fnlwgt          45222 non-null  int64 \n",
      " 3   education       45222 non-null  object\n",
      " 4   education-num   45222 non-null  int64 \n",
      " 5   marital-status  45222 non-null  object\n",
      " 6   occupation      45222 non-null  object\n",
      " 7   relationship    45222 non-null  object\n",
      " 8   race            45222 non-null  object\n",
      " 9   sex             45222 non-null  object\n",
      " 10  capital-gain    45222 non-null  int64 \n",
      " 11  capital-loss    45222 non-null  int64 \n",
      " 12  hours-per-week  45222 non-null  int64 \n",
      " 13  native-country  45222 non-null  object\n",
      " 14  income          45222 non-null  int32 \n",
      "dtypes: int32(1), int64(6), object(8)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking wheter Education and Education-number are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS-grad         14783\n",
      "Some-college     9899\n",
      "Bachelors        7570\n",
      "Masters          2514\n",
      "Assoc-voc        1959\n",
      "Name: education, dtype: int64\n",
      "\n",
      "9     14783\n",
      "10     9899\n",
      "13     7570\n",
      "14     2514\n",
      "11     1959\n",
      "Name: education-num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['education'].value_counts().head())\n",
    "print()\n",
    "print(df['education-num'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From X set, education column is dropped as it is same with Education-num, which is already in numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education-num      marital-status  \\\n",
       "0   39         State-gov   77516             13       Never-married   \n",
       "1   50  Self-emp-not-inc   83311             13  Married-civ-spouse   \n",
       "2   38           Private  215646              9            Divorced   \n",
       "3   53           Private  234721              7  Married-civ-spouse   \n",
       "4   28           Private  338409             13  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  capital-gain  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male          2174   \n",
       "1    Exec-managerial        Husband  White    Male             0   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male             0   \n",
       "3  Handlers-cleaners        Husband  Black    Male             0   \n",
       "4     Prof-specialty           Wife  Black  Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week native-country  income  \n",
       "0             0              40  United-States       0  \n",
       "1             0              13  United-States       0  \n",
       "2             0              40  United-States       0  \n",
       "3             0              40  United-States       0  \n",
       "4             0              40           Cuba       0  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count and percentage of Target classes values after droping NaN values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " 0    34014\n",
      "1    11208\n",
      "Name: income, dtype: int64 \n",
      "\n",
      "Percentage of each class: \n",
      " 0    75.215603\n",
      "1    24.784397\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Number of rows after dropping NaN:  45222\n",
      "number of rows dropped:  3620\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts of unique values in the 'class' column of 'df_class_feature' and store it in 'class_counts'.\n",
    "class_counts = df['income'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each unique value in the 'class' column by dividing 'class_counts' by its sum and then multiplying by 100.\n",
    "class_percentages = class_counts / class_counts.sum() * 100\n",
    "\n",
    "print('Class counts:\\n' ,class_counts, '\\n')\n",
    "print('Percentage of each class: \\n' ,class_percentages)\n",
    "\n",
    "b = df.shape[0]\n",
    "print('\\nNumber of rows after dropping NaN: ', b)\n",
    "print('number of rows dropped: ', a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split data into separate fitting and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y sets.income is the target variable and native country dropped because it has 41 unique values and occupation is very similar too workclass.\n",
    "X = df.drop(['income', 'native-country', 'occupation'], axis=1)\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into fitting data (60%) and test set (40%), the reason I chose 60 to 40 split is to have a bigger test set. I did some experiments with other ratios such as 70 to 30 and 80 to 20 and the model performance metrics were almost the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.3, random_state=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The percentage of each class in the target variable for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a function to calculate  and print the percentage of each class in the target variable\n",
    "def calculate_class_percentage(y):\n",
    "    class_percentage = {}\n",
    "    total_samples = len(y)\n",
    "    unique_classes = set(y)\n",
    "    \n",
    "    for cls in unique_classes:\n",
    "        class_count = sum(y == cls)\n",
    "        percentage = (class_count / total_samples) * 100\n",
    "        class_percentage[cls] = percentage\n",
    "    \n",
    "    return class_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit set class percentages:\n",
      "{0: 75.54572737324278, 1: 24.454272626757227}\n",
      "\n",
      "Test set class percentages:\n",
      "{0: 74.44534532321073, 1: 25.55465467678927}\n",
      "\n",
      " Number of rows of X fit 31655 \n",
      " Number of rows of X test 13567 \n",
      " Number of rows of Y fit 31655 \n",
      " Number of rows of y test 13567\n"
     ]
    }
   ],
   "source": [
    "# Calculate class percentages for each dataset\n",
    "fit_class_percentage = calculate_class_percentage(y_fit)\n",
    "test_class_percentage = calculate_class_percentage(y_test)\n",
    "\n",
    "# Print class percentages for each dataset\n",
    "print(\"Fit set class percentages:\")\n",
    "print(fit_class_percentage )\n",
    "print(\"\\nTest set class percentages:\")\n",
    "print(test_class_percentage)\n",
    "print('\\n Number of rows of X fit', X_fit.shape[0], '\\n Number of rows of X test', X_test.shape[0],'\\n Number of rows of Y fit', y_fit.shape[0],'\\n Number of rows of y test', y_test.shape[0],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest, Logistig Regression, Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score,  recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Random Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF (X_fit, y_fit, X_test, y_test, model_name, i, j , k, estimator, depth):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    model_RF (Pipeline): The trained Random Forest model pipeline.\n",
    "    model_perf_metrics_rf (pandas.DataFrame): The performance metrics of the Random Forest model.\n",
    "    roc_rf (float): The ROC AUC score of the Random Forest model.\n",
    "    f1_rf (float): The F1 score of the Random Forest model.\n",
    "    \"\"\"    \n",
    "    # Preprocessing for cataegorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "        # Preprocessing for numerical data - StandardScaler\n",
    "    #numerical_transformer = Pipeline(steps=[\n",
    "        #('scaler', StandardScaler())])\n",
    "    \n",
    "    # Bundle preprocessing for data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[            \n",
    "            #('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)],\n",
    "        remainder='passthrough')\n",
    "    \n",
    "    # Define the Random Forest model\n",
    "    clf_RF = Pipeline(steps=[('preprocessor', transformations), ('classifier', RandomForestClassifier(n_estimators=estimator, max_depth=depth))])\n",
    "    \n",
    "    # Run the random forest model\n",
    "    model_RF = clf_RF.fit(X_fit, y_fit)\n",
    "    # Predict on the test dataset\n",
    "    y_pred_rf = model_RF.predict(X_test)\n",
    "    # Calculate the needed model performance metrics\n",
    "    f1_rf = f1_score(y_test, y_pred_rf)\n",
    "    roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "    a = len(X_fit)\n",
    "    if depth == None:\n",
    "        # Saving the model performance metrics\n",
    "        models_rf = [('RF_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "    else:\n",
    "        # Saving the model performance metrics\n",
    "        models_rf = [('RF_simp_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metrics_rf = pd.DataFrame(models_rf, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size',  'F1 (%)', 'AUC (%)', 'F1 (%)_std', 'AUC (%)_std'])\n",
    "        \n",
    "    return model_RF, model_perf_metrics_rf, roc_rf, f1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Logistic Regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LR(X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metric_lr (pandas.DataFrame): The performance metrics of the Logistic Regression model.\n",
    "    roc_lr (float): The ROC AUC score of the Logistic Regression model.\n",
    "    f1_lr (float): The F1 score of the Logistic Regression model.\n",
    "    \"\"\"    \n",
    "    # Preprocessing for categorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Preprocessing for numerical data - StandardScaler\n",
    "   # numerical_transformer = Pipeline(steps=[\n",
    "       # ('scaler', StandardScaler())])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "           # ('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)], \n",
    "        remainder='passthrough')\n",
    "\n",
    "    # Define the Logistic Regression model\n",
    "    clf_LR = Pipeline(steps=[('preprocessor', transformations), ('classifier', LogisticRegression(max_iter=1000))])\n",
    "    # Run the Logistic Regression model\n",
    "    model_LR = clf_LR.fit(X_fit, y_fit)\n",
    "    # Predict on the test dataset\n",
    "    y_pred_lr = model_LR.predict(X_test)\n",
    "    # Calculate the needed model performance metrics\n",
    "    f1_lr = f1_score(y_test, y_pred_lr)\n",
    "    roc_lr = roc_auc_score(y_test, y_pred_lr)\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Saving the model performance metrics\n",
    "    models_lr = [('LR_{}'.format(model_name), k, j, i, a, f1_lr, roc_lr, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metric_lr = pd.DataFrame(models_lr, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num','X_fit Size', 'F1 (%)', 'AUC (%)', 'F1 (%)_std', 'AUC (%)_std'])\n",
    "\n",
    "    return  model_perf_metric_lr, roc_lr, f1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_DT(X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "    # Preprocessing for cataegorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "        # Preprocessing for numerical data - StandardScaler\n",
    "    #numerical_transformer = Pipeline(steps=[\n",
    "        #('scaler', StandardScaler())])\n",
    "    \n",
    "    # Bundle preprocessing for data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[            \n",
    "            #('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)],\n",
    "        remainder='passthrough')\n",
    "    \n",
    "        # Initialize the decision tree classifier\n",
    "    clf_DT = Pipeline(steps=[('preprocessor', transformations), ('classifier', DecisionTreeClassifier())])\n",
    "    \n",
    "    # Train the classifier using the fit data\n",
    "    model_DT = clf_DT.fit(X_fit, y_fit)\n",
    "    # Make predictions on the test data\n",
    "    y_pred_dt = model_DT.predict(X_test)\n",
    "\n",
    "    # Calculate the needed model performance metrics\n",
    "    f1_dt = f1_score(y_test, y_pred_dt)\n",
    "    roc_dt = roc_auc_score(y_test, y_pred_dt)\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Saving the model performance metrics\n",
    "    models_dt = [('DT_{}'.format(model_name), k, j, i, a, f1_dt, roc_dt, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metrics_dt = pd.DataFrame(models_dt, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size',  'F1 (%)', 'AUC (%)', 'F1 (%)_std', 'AUC (%)_std'])\n",
    "        \n",
    "    return model_perf_metrics_dt, roc_dt, f1_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models one time and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains Random Forest and Logistic Regression models on the given training data,evaluates their performance on the test data, and merges their performance metrics into two dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (DataFrame): Features for training the models.\n",
    "    y_fit (Series): Target variable for training the models.\n",
    "    X_test (DataFrame): Features for testing the models.\n",
    "    y_test (Series): Target variable for testing the models.\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - model_RF (RandomForestClassifier): The trained Random Forest model.\n",
    "        - model_perf_metrics_merged (DataFrame): Dataframe containing merged performance metrics of the Random Forest and Logistic Regression models.\n",
    "        - model_perf_metrics_merged_only_with_cfs (DataFrame): Another dataframe containing merged performance metrics, identical to the first but intended for use with a counterfactuals dataset.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Running default Random Forest Once\n",
    "    model_RF, model_perf_metric_rf, roc_rf, f1_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0, 100, None)\n",
    "    \n",
    "    # Running default Logistic Regression Once\n",
    "    model_perf_metric_lr, auc, f1 = run_LR(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0)\n",
    "    \n",
    "    # Merging the default model performance metrics in a dataframe to accommodate new models, and doing it twice to use the second one on only CFs dataset.\n",
    "    model_perf_metrics_merged = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    model_perf_metrics_merged_only_with_cfs = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    \n",
    "    return model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models multiple times and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_default_models (k):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation of model performance metrics for default Random Forest and Logistic Regression models over multiple iterations.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): The number of iterations to run the models for calculating the mean and standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained only on CFs.\n",
    "    model_perf_metrics_mean_of_iterations (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained on original data.\n",
    "    \"\"\"\n",
    "    total_auc_rf = []\n",
    "    total_f1_rf = []\n",
    "    total_auc_lr = []\n",
    "    total_f1_lr = []\n",
    "    \n",
    "    for k in range(0, k):    \n",
    "        #Running default RF and LR\n",
    "        _, _ ,auc_def_rf,f1_def_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0, 100, None)\n",
    "        _ , auc_def_lr, f1_def_lr = run_LR(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0)\n",
    "        \n",
    "        # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "        total_auc_rf.append(auc_def_rf)\n",
    "        total_auc_lr.append(auc_def_lr)\n",
    "        total_f1_rf.append(f1_def_rf)\n",
    "        total_f1_lr.append(f1_def_lr)\n",
    "    else:\n",
    "        # Means            \n",
    "        total_auc_rf_mean = np.mean(total_auc_rf)\n",
    "        total_f1_rf_mean = np.mean(total_f1_rf)\n",
    "\n",
    "        total_auc_lr_mean = np.mean(total_auc_lr)\n",
    "        total_f1_lr_mean = np.mean(total_f1_lr)\n",
    "\n",
    "        # Standart  deviations\n",
    "        total_auc_rf_std = round(np.std(total_auc_rf),8)\n",
    "        total_f1_rf_std = round(np.std(total_f1_rf),8)\n",
    "\n",
    "        total_auc_lr_std = round(np.std(total_auc_lr),8)\n",
    "        total_f1_lr_std = round(np.std(total_f1_lr),8)\n",
    "\n",
    "        c = len(X_fit)\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = pd.DataFrame()\n",
    "        model_perf_metrics_mean_of_iterations = pd.DataFrame()\n",
    "\n",
    "        default_RF_means_row = {'Model': 'RF_default_mean_of_iterations', 'Iteration': k+1, 'Sample Size': 0, 'CF Num': 0, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_rf_mean, 'F1 (%)_std':total_f1_rf_std, 'AUC (%)' :total_auc_rf_mean, 'AUC (%)_std': total_auc_rf_std}\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_RF_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_RF_means_row, ignore_index=True)   \n",
    "       \n",
    "        default_LR_means_row = {'Model': 'LR_default_mean_of_iterations', 'Iteration': k+1, 'Sample Size': 0, 'CF Num': 0, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_lr_mean, 'F1 (%)_std':total_f1_lr_std, 'AUC (%)' :total_auc_lr_mean, 'AUC (%)_std': total_auc_lr_std}\n",
    "\n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_LR_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_LR_means_row, ignore_index=True)\n",
    "    \n",
    "        return model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the means and standart deviations of multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of model performance metrics for each iterations and constructs summary rows for inclusion in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    total_auc_cf (list): AUC scores for models trained only on counterfactuals across iterations.\n",
    "    total_f1_cf (list): F1 scores for models trained only on counterfactuals across iterations.\n",
    "    total_auc (list): AUC scores for models trained on original data plus counterfactuals across iterations.\n",
    "    total_f1 (list): F1 scores for models trained on original data plus counterfactuals across iterations.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    a (int): The size of the training dataset including original data plus counterfactuals.\n",
    "    b (int): The size of the training dataset with only counterfactuals.\n",
    "\n",
    "    Returns:\n",
    "    new_row (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained on original data plus counterfactuals.\n",
    "    new_row_cf (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained only on counterfactuals.\n",
    "    \"\"\"\n",
    "        \n",
    "    total_auc_only_cf_mean = np.mean(total_auc_cf)\n",
    "    total_f1_only_cf_mean = np.mean(total_f1_cf)\n",
    "\n",
    "    total_auc_mean = np.mean(total_auc)\n",
    "    total_f1_mean = np.mean(total_f1)\n",
    "\n",
    "    #Standart  deviations\n",
    "    total_auc_only_cf_std = round(np.std(total_auc_cf),8)\n",
    "    total_f1_only_cf_std = round(np.std(total_f1_cf),8)\n",
    "\n",
    "    total_auc_std = round(np.std(total_auc),8)\n",
    "    total_f1_std = round(np.std(total_f1),8)\n",
    "    \n",
    "    new_row   = {'Model': 'LR_sample:{}_cf:{}_mean_of_{}_iterations'.format(j, i, k), 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':a ,\n",
    "                             'F1 (%)': total_f1_mean, 'F1 (%)_std':total_f1_std, 'AUC (%)' :total_auc_mean, 'AUC (%)_std': total_auc_std}\n",
    " \n",
    "                # Creating new rows to save the model performance metrics of each sample size and CF number combination's all iterations\n",
    "    new_row_cf= {'Model': 'LR_sample:{}_cf:{}_mean_of_{}_iterations_only_cf'.format(j, i, k) , 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':b , \n",
    "                             'F1 (%)': total_f1_only_cf_mean, 'F1 (%)_std': total_f1_only_cf_std, 'AUC (%)' :total_auc_only_cf_mean, 'AUC (%)_std': total_auc_only_cf_std}\n",
    "    return new_row, new_row_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0022113130777055416]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # Get the sample size, CF Num and Iteration values    \n",
    "x = df['income'].unique()\n",
    "percentages = [(value / df['income'].count()) * 100 for value in x]\n",
    "\n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrubution_plot(df, name, true_false_1, true_false_2):\n",
    "    \"\"\"\n",
    "    This function plots the distribution of scores for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the model performance data.\n",
    "    name (str): The list of the Scores that will be plotted.\n",
    "    1st True False is a flag to determine if the data is mean of iterations or not.\n",
    "    2nd True False is a flag to determine if the data is only CFs or CFs + original data.    \n",
    "    \"\"\"\n",
    "    # Get the default values of the models\n",
    "    lr_line = df[df['Model'].str.startswith('LR_default')][name].iloc[0]\n",
    "    rf_line = df[df['Model'].str.startswith('RF_default')][name].iloc[0]\n",
    "    df = df[(df['Model'] != 'RF_default') & (df['Model'] != 'LR_default')]\n",
    "    df = df[(df['Model'] != 'RF_default_mean_of_iterations') & (df['Model'] != 'LR_default_mean_of_iterations')]\n",
    "\n",
    "    # Get the sample size, CF Num and Iteration values    \n",
    "    x = df['Sample Size'].unique()\n",
    "    x_2 = df['CF Num'].unique()\n",
    "    x_3 = df['Iteration'].unique()\n",
    "\n",
    "    # Get the y values\n",
    "    y = df[name]\n",
    "    y_err = df[name+'_std']\n",
    "    # combine x and x_2 for x-axis labels\n",
    "    combined_x = [f\"{i}_{j}_{k}\" for i in x for j in x_2 for k in x_3]\n",
    "\n",
    "    # plot lines\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    \n",
    "    # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "    # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs\n",
    "    if true_false_1 is True:\n",
    "        plt.scatter(combined_x, y, color=\"blue\")\n",
    "        plt.errorbar(combined_x, y, yerr=y_err, fmt='o', color=\"blue\",  capsize=6, label=\"{} Values\".format(name))\n",
    "\n",
    "        plt.axhline(y=rf_line, color='r', linestyle='--', label=\"Default_RF_Line_Mean_of_{}\".format(x_3))  # Plot RF_line as a horizontal line\n",
    "        plt.axhline(y=lr_line, color='green', linestyle='--', label=\"Default_LR_line_Mean_of_{}\".format(x_3))  # Plot LR_line as a horizontal line\n",
    "\n",
    "        plt.ylabel('{} score'.format(name))\n",
    "        plt.xlabel('Sample Size- CF Num- Iteration')\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.05, 1.155))\n",
    "        plt.xticks(rotation=40)  # rotate x-axis labels for better visibility        \n",
    "        \n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models\\n Trained on CFs\\nSample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "        else:\n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "    else:\n",
    "        plt.plot(combined_x, y, 'o-', color=\"blue\", label=\"{} Values\".format(name))\n",
    "        \n",
    "        plt.axhline(y=rf_line, color='r', linestyle='--', label=\"Default_RF_Line\")  # Plot RF_line as a horizontal line\n",
    "        plt.axhline(y=lr_line, color='green', linestyle='--', label=\"Default_LR_line\")  # Plot LR_line as a horizontal line\n",
    "\n",
    "        plt.ylabel('{} score'.format(name))\n",
    "        plt.xlabel('Sample Size- CF Num- Iteration')\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.05, 1.155))\n",
    "        plt.xticks(rotation=40)  # rotate x-axis labels for better visibility    \n",
    "\n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models \\nTrained on CFs\\nSample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "        else:    \n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "            \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the DICE model. Possible adjustments that can be made are, \n",
    "- which model to use for CF generation, \n",
    "- continuous features, \n",
    "- size of the sample that will be used to create the CFs, \n",
    "- how many CFs to generate per one sample,\n",
    "- which features can vary in the CF generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DICE model\n",
    "def generate_counterfactuals(X_fit, y_fit, model, sample_size, total_CFs, fea_to_vary, outcome_name, k ):\n",
    "    \"\"\"\n",
    "    Generates counterfactual examples using the DICE (Diverse Counterfactual Explanations) method.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    model (object): The machine learning model for which counterfactuals are to be generated.\n",
    "    sample_size (int): The number of samples for which to generate counterfactuals.\n",
    "    total_CFs (int): The total number of counterfactuals to generate per sample.\n",
    "    fea_to_vary (list): The list of features to be varied in the counterfactual generation process.\n",
    "    outcome_name (str): The name of the outcome variable.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    X_fit_cf (pandas.DataFrame): The feature set of the generated counterfactuals.\n",
    "    y_fit_cf (pandas.Series): The target variable of the generated counterfactuals.\n",
    "    cf_df (pandas.DataFrame): A dataframe containing all the generated counterfactuals.\n",
    "    \"\"\"\n",
    "    all_columns = X_fit.columns\n",
    "    # Calculate the original class distribution\n",
    "    class_0_count = np.sum(y_fit == 0)\n",
    "    class_1_count = np.sum(y_fit == 1)\n",
    "\n",
    "    # Calculate the opposite class distribution proportions\n",
    "    total_count = class_0_count + class_1_count\n",
    "    prop_class_0 = class_1_count / total_count\n",
    "    prop_class_1 = class_0_count / total_count\n",
    "\n",
    "    # Determine the number of samples for each class in the opposite distribution\n",
    "    total_samples = sample_size\n",
    "    num_class_0_samples = int(total_samples * prop_class_0)\n",
    "    num_class_1_samples = total_samples - num_class_0_samples  # Ensuring total is 1000\n",
    "\n",
    "    # Sample from each class to get the opposite distribution\n",
    "    X_class_0 = X_fit[y_fit == 0]\n",
    "    X_class_1 = X_fit[y_fit == 1]\n",
    "\n",
    "    y_class_0 = y_fit[y_fit == 0]\n",
    "    y_class_1 = y_fit[y_fit == 1]\n",
    "\n",
    "    # Sample the required number of samples from each class\n",
    "    X_class_0_sampled, _, y_class_0_sampled, _ = train_test_split(X_class_0, y_class_0, train_size=num_class_0_samples, random_state=42)\n",
    "    X_class_1_sampled, _, y_class_1_sampled, _ = train_test_split(X_class_1, y_class_1, train_size=num_class_1_samples, random_state=42)\n",
    "\n",
    "    # Assuming X_class_0_sampled, X_class_1_sampled are your numpy arrays with same number of columns\n",
    "    X_fit = pd.DataFrame(np.concatenate([X_class_0_sampled, X_class_1_sampled]), columns=all_columns)  # replace column names with your actual column names\n",
    "    \n",
    "    # Assuming y_class_0_sampled, y_class_1_sampled are your numpy arrays with single column\n",
    "    y_fit = pd.DataFrame(np.concatenate([y_class_0_sampled, y_class_1_sampled]), columns=[outcome_name])  # assuming y_fit is a target variable\n",
    "\n",
    "    # Optionally, shuffle the dataset to mix the classes\n",
    "    X_fit, y_fit = shuffle(X_fit, y_fit, random_state=42)\n",
    "\n",
    "    # Create a Dice-ML data object\n",
    "    d = dice_ml.Data(dataframe=X_fit.assign(income=y_fit), continuous_features=[], outcome_name=outcome_name)\n",
    "    # Creating a Model object\n",
    "    m = dice_ml.Model(model=model, backend=\"sklearn\")\n",
    "    # Creating the DICE model with data and model object \n",
    "    exp = dice_ml.Dice(d, m, method=\"random\")\n",
    "\n",
    "    # Execution of the DICE model to generate counterfactuals\n",
    "    e1 = exp.generate_counterfactuals(X_fit, total_CFs=total_CFs, desired_class=\"opposite\", features_to_vary= fea_to_vary)\n",
    "    \n",
    "    # Comment out the below line as CFs needed to be visualized in the notebook\n",
    "    #e1.visualize_as_dataframe(show_only_changes=True)\n",
    "\n",
    "    # Creating a new empty dataframe to store the generated CFs\n",
    "    cf_df = pd.DataFrame()\n",
    "    # For loop to append the counterfactuals of each sample, to a dataframe\n",
    "    for i in range(0, sample_size):\n",
    "        # Saving CFs\n",
    "        xd = e1.cf_examples_list[i].final_cfs_df\n",
    "        # Adding the generated CFs to the empty dataframe\n",
    "        cf_df = pd.concat([cf_df, xd])\n",
    "# If the sample size is reached and the CFs are generated for all the samples, followiing code resets the index of the dataframe and saves the generated CFs as a csv file.\n",
    "    else:\n",
    "        # Resetting the index of the dataframe to not have duplicate index number\n",
    "        cf_df.reset_index(drop=True, inplace=True)\n",
    "        new_start_index = len(X_fit) + i + len(cf_df)\n",
    "        cf_df.index += new_start_index \n",
    "        # Saving the generated CFs as a csv file\n",
    "        cf_df.to_csv('Generated_CFs/cf_df_{}_{}_{}.csv'.format(sample_size, total_CFs,k ))\n",
    "        \n",
    "        # Following code creates X and y fit sets for the generated CFs and returns them for further use.\n",
    "        X_fit_cf = cf_df.drop([outcome_name], axis=1)\n",
    "        y_fit_cf = cf_df[outcome_name]\n",
    "\n",
    "    return X_fit_cf, y_fit_cf, cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a experiment  fuction that runs the RF and LR, DICE model that generates counterfactuals, and then using those CFs for more model traning. \n",
    "#### Then the function saves these CF explanations into a dataframe and trains the LR again with the generated CFs. At the same time, model appends the original fit-set to CF dataset and trains the LR again with the combined version of the datasets. For each model training, LR model function appends the new model performance metrics into a dataframe. Possible adjustments that can be made are, \n",
    "- continuous features, \n",
    "- size of the sample that will be used to create the CFs, \n",
    "- how many CFs to generate per one sample,\n",
    "- how many iterations will be made for each sample_size and CF_number pair,\n",
    "- which features can vary in the CF generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment running function\n",
    "def running_exp (X_fit, y_fit, X_test, y_test,  sample_size_list, total_CFs_list, iteration_num, fea_to_vary, name_of_the_exp, list_of_models, target_variable):\n",
    "    \"\"\"\n",
    "    This function is for running the experiments with different sample sizes, CF numbers and iterations.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    sample_size_list (list): The list of sample sizes to be used in the experiments.\n",
    "    total_CFs_list (list): The list of total CF numbers to be generated for each sample size.\n",
    "    iteration_num (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    fea_to_vary (list): The list of features to be varied in the CF generation process. Other features will be kept constant.\n",
    "    num (int): The number to be added to the file names to distinguish the results of different experiments.\n",
    "    list_of_models (list): The names of the Scores to be plotted. For example, AUC or F1.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_merged_with_for_cfs (pandas.DataFrame): The model performance metrics of the experiments trained with only CFs.\n",
    "    model_perf_metrics_merged (pandas.DataFrame): The model performance metrics of the experiments trained with CFs and original data.\n",
    "    new_cf_df (pandas.DataFrame): All generated CFs in every CF number, sample size and iterations.\n",
    "    \"\"\"\n",
    "    new_cf_df = pd.DataFrame()\n",
    "\n",
    "    # Running the default Random Forest and Logistic Regression models once and merging their performance metrics in a dataframe.\n",
    "    model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs = run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test)\n",
    "    \n",
    "    # Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations = mean_of_default_models (iteration_num)\n",
    "    \n",
    "    # Saving these new rows to the model performance metrics dataframes    \n",
    "    # Loop through different sample sizes\n",
    "    for j in sample_size_list:\n",
    "        # Loop through different total CF numbers\n",
    "        for i in total_CFs_list:\n",
    "            # Initializing empty lists for saving auc and f1 scores for each iteration\n",
    "            total_f1_cf = []\n",
    "            total_f1 = []\n",
    "            total_auc_cf = []\n",
    "            total_auc = []\n",
    "\n",
    "            # Loop through iteration numbers, doing a new experiment for each sample_size and CF_number combination\n",
    "            for k in range(1,iteration_num+1):    \n",
    "                # Generating counterfactuals using the 'generate_counterfactuals' function\n",
    "                X_fit_cf, y_fit_cf, cf_df = generate_counterfactuals(X_fit, y_fit, model_RF, j, i, fea_to_vary, target_variable, k)\n",
    "                \n",
    "                # Concatenating the original dataset with the generated CFs dataset\n",
    "                new_cf_df = new_cf_df.append(cf_df)\n",
    "                new_X_fit = pd.concat([X_fit, X_fit_cf])\n",
    "                new_y_fit = pd.concat([y_fit, y_fit_cf])\n",
    "\n",
    "                # Running the Logistic Regression model on the CFs dataset and the dataset with original X_fit plus CFs.\n",
    "                model_perf_metric_lr_only_cf, auc_only_cf, f1_only_cf = run_LR(X_fit_cf, y_fit_cf, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                model_perf_metric_lr_cf_org, auc_lr_cf, f1_lr_cf = run_LR(new_X_fit, new_y_fit, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)                \n",
    "                \n",
    "                ''' Running the Random Forest model on the CFs dataset and the dataset with original X_fit plus CFs.\n",
    "                _,model_perf_metric_lr_only_cf, auc_only_cf, f1_only_cf = run_RF(X_fit_cf, y_fit_cf, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                _, model_perf_metric_lr_cf_org, auc_rf_cf, f1_rf_cf = run_RF(new_X_fit, new_y_fit, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)'''\n",
    "\n",
    "                # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "                total_f1_cf.append(f1_only_cf)\n",
    "                total_f1.append(f1_lr_cf)\n",
    "                total_auc_cf.append(auc_only_cf)\n",
    "                total_auc.append(auc_lr_cf)\n",
    "\n",
    "                # Saving the model performance metrics of each iteration\n",
    "                model_perf_metrics_merged_only_with_cfs = model_perf_metrics_merged_only_with_cfs.append(model_perf_metric_lr_only_cf, ignore_index=True)\n",
    "                model_perf_metrics_merged = model_perf_metrics_merged.append(model_perf_metric_lr_cf_org, ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "            # Calculate means and standard deviations of auc and f1 scores of all the iterations for each sample size and CF number combination\n",
    "                a = len(new_X_fit)\n",
    "                b = len(X_fit_cf)\n",
    "\n",
    "                new_row, new_row_cf = calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b)\n",
    "            \n",
    "                # Saving these new rows to the model performance metrics dataframes    \n",
    "                model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(new_row_cf, ignore_index=True)\n",
    "                model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(new_row, ignore_index=True)\n",
    "                \n",
    "    # Saving the model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_merged_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_merged_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_merged.to_excel('Performance_metrics/model_perf_metrics_merged_{}.xlsx'.format(name_of_the_exp))\n",
    "    # Saving the MEAN model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_mean_of_iterations.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_{}.xlsx'.format(name_of_the_exp))\n",
    "    \n",
    "    for k in list_of_models:\n",
    "        # k is the list of the Scores to be plotted\n",
    "        # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "        # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs'''\n",
    "        distrubution_plot(model_perf_metrics_merged, k, False, False )\n",
    "        distrubution_plot(model_perf_metrics_merged_only_with_cfs, k, False, True )\n",
    "\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations, k, True , False)\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations_only_with_cfs, k, True, True )\n",
    "        \n",
    "    return  model_perf_metrics_merged_only_with_cfs, model_perf_metrics_merged, new_cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13567 entries, 14159 to 23285\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             13567 non-null  int64 \n",
      " 1   workclass       13567 non-null  object\n",
      " 2   fnlwgt          13567 non-null  int64 \n",
      " 3   education-num   13567 non-null  int64 \n",
      " 4   marital-status  13567 non-null  object\n",
      " 5   relationship    13567 non-null  object\n",
      " 6   race            13567 non-null  object\n",
      " 7   sex             13567 non-null  object\n",
      " 8   capital-gain    13567 non-null  int64 \n",
      " 9   capital-loss    13567 non-null  int64 \n",
      " 10  hours-per-week  13567 non-null  int64 \n",
      "dtypes: int64(6), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "X_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the columns' categories\n",
    "numerical = ['age' ,'fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical = ['workclass','marital-status',  'relationship', 'race', 'sex']\n",
    "target_variable = 'income'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_to_vary = ['age','fnlwgt', 'workclass', 'education-num', 'marital-status', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "sample_size_list = [6331, 9495]\n",
    "total_CFs_list = [1, 2]\n",
    "iteration_num = 5\n",
    "list_of_plots = ['AUC (%)'] \n",
    "model_perf_metrics_merged_only_with_cfs, model_perf_metrics_merged, neww = running_exp(X_fit, y_fit, X_test, y_test,  sample_size_list, total_CFs_list, iteration_num, fea_to_vary, '6331_9495__1_2 ', list_of_plots,target_variable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
