{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd  # For data manipulation\n",
    "import numpy as np  # For numerical operations\n",
    "\n",
    "# Importing preprocessing and model selection modules from sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split  # For splitting the data into train and test sets\n",
    "from sklearn.preprocessing import LabelEncoder  # For encoding categorical labels\n",
    "from sklearn.preprocessing import OneHotEncoder  # For one-hot encoding categorical features\n",
    "import matplotlib.pyplot as plt  # For data visualization\n",
    "import warnings  # To handle warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignoring future warnings\n",
    "from sklearn.utils import shuffle  # For shuffling the data\n",
    "\n",
    "# Importing additional modules from sklearn\n",
    "from sklearn.compose import ColumnTransformer  # For applying different preprocessing to different columns\n",
    "from sklearn.model_selection import train_test_split  # Reimported train_test_split (duplicate import)\n",
    "from sklearn.pipeline import Pipeline  # For creating a pipeline of transformations and models\n",
    "from sklearn.preprocessing import OneHotEncoder  # Reimported OneHotEncoder (duplicate import)\n",
    "from sklearn.ensemble import RandomForestClassifier  # For random forest model\n",
    "from sklearn.linear_model import LogisticRegression  # For logistic regression model\n",
    "from sklearn.preprocessing import StandardScaler  # For standardizing features\n",
    "from sklearn.tree import DecisionTreeClassifier  # For decision tree model\n",
    "\n",
    "# Importing DiCE (Diverse Counterfactual Explanations) library\n",
    "import dice_ml  # For generating counterfactual explanations\n",
    "from dice_ml.utils import helpers  # Importing helper functions from DiCE\n",
    "\n",
    "# Importing additional libraries\n",
    "import pickle  # For saving and loading models\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc  # For evaluating model performance\n",
    "from sklearn.model_selection import cross_val_score, KFold  # For cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from a csv file\n",
    "df = pd.read_csv('CensusAdultIncome.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48842 entries, 0 to 48841\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             48842 non-null  int64 \n",
      " 1   workclass       47879 non-null  object\n",
      " 2   fnlwgt          48842 non-null  int64 \n",
      " 3   education       48842 non-null  object\n",
      " 4   education-num   48842 non-null  int64 \n",
      " 5   marital-status  48842 non-null  object\n",
      " 6   occupation      47876 non-null  object\n",
      " 7   relationship    48842 non-null  object\n",
      " 8   race            48842 non-null  object\n",
      " 9   sex             48842 non-null  object\n",
      " 10  capital-gain    48842 non-null  int64 \n",
      " 11  capital-loss    48842 non-null  int64 \n",
      " 12  hours-per-week  48842 non-null  int64 \n",
      " 13  native-country  48568 non-null  object\n",
      " 14  income          48842 non-null  object\n",
      "dtypes: int64(6), object(9)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- workclass: A categorical feature representing the type of income, such as private, self-employment, and government employment. Some missing values present.\n",
    "- fnlwgt: An integer feature with no description provided. No missing values.\n",
    "- education: A categorical feature representing the level of education \n",
    "- education-num: An integer feature representing the numerical encoding of **education** level.\n",
    "- occupation: A categorical feature representing the type of occupation, such as managerial, technical, and service-related occupations. Some missing values present.\n",
    "- native-country: A categorical feature representing the country of origin, including various countries such as the United States, Canada, and India. Some missing values present.\n",
    "- income: The target variable, a binary feature representing income level, with categories >50K and <=50K. No missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cleaning the target variable and making it binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target value has values with dots, therefore 4 catagories\n",
    "df['income'] = df['income'].str.replace('.', '')\n",
    "\n",
    "# Replace the values in the target column with string '0' and '1'.\n",
    "df['income'] = df['income'].str.replace('<=50K', '0')  \n",
    "df['income'] = df['income'].str.replace('>50K', '1')   \n",
    "\n",
    "# Convert to integer\n",
    "df['income'] = df['income'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print count and percentage of classes variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " 0    37155\n",
      "1    11687\n",
      "Name: income, dtype: int64 \n",
      "\n",
      "Percentage of each class: \n",
      " 0    76.071823\n",
      "1    23.928177\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Total number of rows:  48842\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts of unique values in the 'class' column \n",
    "class_counts = df['income'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each unique value in the 'class' column by dividing 'class_counts' by its sum and then multiplying by 100.\n",
    "class_percentages = class_counts / class_counts.sum() * 100\n",
    "\n",
    "print('Class counts:\\n' ,class_counts, '\\n')\n",
    "print('Percentage of each class: \\n' ,class_percentages)\n",
    "print('\\nTotal number of rows: ', df.shape[0])\n",
    "\n",
    "# Saving this for future use\n",
    "a = df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Droping nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some values are like '?'. Replacing them as NaN \n",
    "df[df == '?'] = np.nan\n",
    "# Dropping the rows with NaN values in  'workclass', 'occupation', 'native-country' for the test dataset\n",
    "df.dropna(subset=['workclass', 'occupation', 'native-country'], inplace=True)\n",
    "df.dropna( inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking wheter Education and Education-number are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HS-grad         14783\n",
      "Some-college     9899\n",
      "Bachelors        7570\n",
      "Masters          2514\n",
      "Assoc-voc        1959\n",
      "Name: education, dtype: int64\n",
      "\n",
      "9     14783\n",
      "10     9899\n",
      "13     7570\n",
      "14     2514\n",
      "11     1959\n",
      "Name: education-num, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['education'].value_counts().head())\n",
    "print()\n",
    "print(df['education-num'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From X set, education column is dropped as it is same with Education-num, which is already in numerical form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education-num      marital-status  \\\n",
       "0   39         State-gov   77516             13       Never-married   \n",
       "1   50  Self-emp-not-inc   83311             13  Married-civ-spouse   \n",
       "2   38           Private  215646              9            Divorced   \n",
       "3   53           Private  234721              7  Married-civ-spouse   \n",
       "4   28           Private  338409             13  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  capital-gain  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male          2174   \n",
       "1    Exec-managerial        Husband  White    Male             0   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male             0   \n",
       "3  Handlers-cleaners        Husband  Black    Male             0   \n",
       "4     Prof-specialty           Wife  Black  Female             0   \n",
       "\n",
       "   capital-loss  hours-per-week native-country  income  \n",
       "0             0              40  United-States       0  \n",
       "1             0              13  United-States       0  \n",
       "2             0              40  United-States       0  \n",
       "3             0              40  United-States       0  \n",
       "4             0              40           Cuba       0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count and percentage of Target classes values after droping NaN values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class counts:\n",
      " 0    34014\n",
      "1    11208\n",
      "Name: income, dtype: int64 \n",
      "\n",
      "Percentage of each class: \n",
      " 0    75.215603\n",
      "1    24.784397\n",
      "Name: income, dtype: float64\n",
      "\n",
      "Number of rows after dropping NaN:  45222\n",
      "number of rows dropped:  3620\n"
     ]
    }
   ],
   "source": [
    "# Calculate the counts of unique values in the 'class' column of 'df_class_feature' and store it in 'class_counts'.\n",
    "class_counts = df['income'].value_counts()\n",
    "\n",
    "# Calculate the percentage of each unique value in the 'class' column by dividing 'class_counts' by its sum and then multiplying by 100.\n",
    "class_percentages = class_counts / class_counts.sum() * 100\n",
    "\n",
    "print('Class counts:\\n' ,class_counts, '\\n')\n",
    "print('Percentage of each class: \\n' ,class_percentages)\n",
    "\n",
    "b = df.shape[0]\n",
    "print('\\nNumber of rows after dropping NaN: ', b)\n",
    "print('number of rows dropped: ', a-b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split data into separate fitting and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating X and y sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y sets.income is the target variable and native country dropped because it has 41 unique values and occupation is very similar too workclass.\n",
    "X = df.drop(['income', 'native-country', 'occupation'], axis=1)\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into fitting data (70%) and test set (30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and testing sets\n",
    "# X: Features dataset\n",
    "# y: Target variable\n",
    "# test_size=0.3: 30% of the data will be used for testing, 70% for training\n",
    "# random_state=1600: Ensures reproducibility of the split\n",
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.3, random_state=1600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The percentage of each class in the target variable for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined a function to calculate and print the percentage of each class in the target variable\n",
    "def calculate_class_percentage(y):\n",
    "    # Initialize an empty dictionary to store class percentages\n",
    "    class_percentage = {}\n",
    "    \n",
    "    # Calculate the total number of samples in the target variable\n",
    "    total_samples = len(y)\n",
    "    \n",
    "    # Get the unique classes in the target variable\n",
    "    unique_classes = set(y)\n",
    "    \n",
    "    # Iterate over each unique class\n",
    "    for cls in unique_classes:\n",
    "        # Count the number of occurrences of the current class in the target variable\n",
    "        class_count = sum(y == cls)\n",
    "        \n",
    "        # Calculate the percentage of the current class\n",
    "        percentage = (class_count / total_samples) * 100\n",
    "        \n",
    "        # Store the percentage in the dictionary with the class as the key\n",
    "        class_percentage[cls] = percentage\n",
    "    \n",
    "    # Return the dictionary containing class percentages\n",
    "    return class_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit set class percentages:\n",
      "{0: 75.54572737324278, 1: 24.454272626757227}\n",
      "\n",
      "Test set class percentages:\n",
      "{0: 74.44534532321073, 1: 25.55465467678927}\n",
      "\n",
      " Number of rows of X fit 31655 \n",
      " Number of rows of X test 13567 \n",
      " Number of rows of Y fit 31655 \n",
      " Number of rows of y test 13567\n"
     ]
    }
   ],
   "source": [
    "# Calculate class percentages for each dataset\n",
    "fit_class_percentage = calculate_class_percentage(y_fit)\n",
    "test_class_percentage = calculate_class_percentage(y_test)\n",
    "\n",
    "# Print class percentages for each dataset\n",
    "print(\"Fit set class percentages:\")\n",
    "print(fit_class_percentage )\n",
    "print(\"\\nTest set class percentages:\")\n",
    "print(test_class_percentage)\n",
    "print('\\n Number of rows of X fit', X_fit.shape[0], '\\n Number of rows of X test', X_test.shape[0],'\\n Number of rows of Y fit', y_fit.shape[0],'\\n Number of rows of y test', y_test.shape[0],)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest, Logistig Regression, Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Random Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF(X_fit, y_fit, X_test, y_test, model_name, i, j, k, estimator, depth):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    estimator (int): The number of trees in the forest.\n",
    "    depth (int or None): The maximum depth of the tree. If None, nodes are expanded until all leaves are pure.\n",
    "\n",
    "    Returns:\n",
    "    model_RF (Pipeline): The trained Random Forest model pipeline.\n",
    "    model_perf_metrics_rf (pandas.DataFrame): The performance metrics of the Random Forest model.\n",
    "    roc_rf (float): The ROC AUC score of the Random Forest model.\n",
    "    f1_rf (float): The F1 score of the Random Forest model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocessing for categorical data using OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Define the transformations for preprocessing the data\n",
    "    # Commented out the numerical transformer since it's not currently used\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "         ('scaler', StandardScaler())])\n",
    "\n",
    "    # Combine preprocessing steps into a single transformer\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical),  # Placeholder for numerical transformer\n",
    "            ('cat', categorical_transformer, categorical)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "    # Define the Random Forest model as a pipeline\n",
    "    clf_RF = Pipeline(steps=[('preprocessor', transformations), \n",
    "                             ('classifier', RandomForestClassifier(n_estimators=estimator, max_depth=depth))])\n",
    "\n",
    "    # Fit the Random Forest model on the training data\n",
    "    model_RF = clf_RF.fit(X_fit, y_fit)\n",
    "\n",
    "    # Predict on the test dataset\n",
    "    y_pred_rf = model_RF.predict(X_test)\n",
    "\n",
    "    # Calculate the F1 score and ROC AUC score\n",
    "    f1_rf = f1_score(y_test, y_pred_rf)\n",
    "    roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "\n",
    "    # Get the size of the training data\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Saving the model performance metrics\n",
    "    if depth is None:\n",
    "        # If no depth is specified, use a different model name format\n",
    "        models_rf = [('RF_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "    else:\n",
    "        # Use a simplified model name format if depth is specified\n",
    "        models_rf = [('RF_simp_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "\n",
    "    # Create a DataFrame to store the performance metrics\n",
    "    model_perf_metrics_rf = pd.DataFrame(models_rf, \n",
    "                                         columns=['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size', \n",
    "                                                  'F1 (%)', 'AUC (%)', 'F1 (%)_std', 'AUC (%)_std'])\n",
    "\n",
    "    return model_RF, model_perf_metrics_rf, roc_rf, f1_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_DT(X_fit, y_fit, X_test, y_test, model_name, i, j, k):\n",
    "    \"\"\"\n",
    "    Trains a Decision Tree model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_dt (pandas.DataFrame): The performance metrics of the Decision Tree model.\n",
    "    roc_dt (float): The ROC AUC score of the Decision Tree model.\n",
    "    f1_dt (float): The F1 score of the Decision Tree model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocessing for categorical data using OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # Preprocessing for numerical data using StandardScaler\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    # Bundle preprocessing for data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # Initialize the Decision Tree classifier\n",
    "    clf_DT = Pipeline(steps=[('preprocessor', transformations), \n",
    "                             ('classifier', DecisionTreeClassifier())])\n",
    "    \n",
    "    # Train the classifier using the fit data\n",
    "    model_DT = clf_DT.fit(X_fit, y_fit)\n",
    "    \n",
    "    # Make predictions on the test data\n",
    "    y_pred_dt = model_DT.predict(X_test)\n",
    "\n",
    "    # Calculate the F1 score and ROC AUC score\n",
    "    f1_dt = f1_score(y_test, y_pred_dt)\n",
    "    roc_dt = roc_auc_score(y_test, y_pred_dt)\n",
    "    \n",
    "    # Get the size of the training data\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Save the model performance metrics\n",
    "    models_dt = [('DT_{}'.format(model_name), k, j, i, a, f1_dt, roc_dt, 0, 0)]\n",
    "    \n",
    "    # Save the model performance metrics in a dataframe\n",
    "    model_perf_metrics_dt = pd.DataFrame(models_dt, \n",
    "                                         columns=['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size', \n",
    "                                                  'F1 (%)', 'AUC (%)', 'F1 (%)_std', 'AUC (%)_std'])\n",
    "        \n",
    "    return model_perf_metrics_dt, roc_dt, f1_dt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models one time and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains Random Forest and Logistic Regression models on the given training data,evaluates their performance on the test data, and merges their performance metrics into two dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (DataFrame): Features for training the models.\n",
    "    y_fit (Series): Target variable for training the models.\n",
    "    X_test (DataFrame): Features for testing the models.\n",
    "    y_test (Series): Target variable for testing the models.\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - model_RF (RandomForestClassifier): The trained Random Forest model.\n",
    "        - model_perf_metrics_merged (DataFrame): Dataframe containing merged performance metrics of the Random Forest and Logistic Regression models.\n",
    "        - model_perf_metrics_merged_only_with_cfs (DataFrame): Another dataframe containing merged performance metrics, identical to the first but intended for use with a counterfactuals dataset.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Running default Random Forest Once\n",
    "    model_RF, model_perf_metric_rf, roc_rf, f1_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0, 100, None)\n",
    "    \n",
    "    # Running default Decision Tree Once\n",
    "    model_perf_metric_lr, auc, f1 = run_DT(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0)\n",
    "    \n",
    "    # Merging the default model performance metrics in a dataframe to accommodate new models, and doing it twice to use the second one on only CFs dataset.\n",
    "    model_perf_metrics_merged = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    model_perf_metrics_merged_only_with_cfs = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    \n",
    "    return model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models multiple times and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_default_models (k):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation of model performance metrics for default Random Forest and Logistic Regression models over multiple iterations.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): The number of iterations to run the models for calculating the mean and standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained only on CFs.\n",
    "    model_perf_metrics_mean_of_iterations (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained on original data.\n",
    "    \"\"\"\n",
    "    total_auc_rf = []\n",
    "    total_f1_rf = []    \n",
    "    total_auc_lr = []\n",
    "    total_f1_lr = []\n",
    "    \n",
    "    for k in range(0, k):    \n",
    "        #Running default RF and LR\n",
    "        _, _ ,auc_def_rf,f1_def_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0, 100, None)\n",
    "        _ , auc_def_lr, f1_def_lr = run_DT(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0)\n",
    "        \n",
    "        # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "        total_auc_rf.append(auc_def_rf)\n",
    "        total_auc_lr.append(auc_def_lr)\n",
    "        total_f1_rf.append(f1_def_rf)\n",
    "        total_f1_lr.append(f1_def_lr)\n",
    "    else:\n",
    "        # Means            \n",
    "        total_auc_rf_mean = np.mean(total_auc_rf)\n",
    "        total_f1_rf_mean = np.mean(total_f1_rf)\n",
    "\n",
    "        total_auc_lr_mean = np.mean(total_auc_lr)\n",
    "        total_f1_lr_mean = np.mean(total_f1_lr)\n",
    "\n",
    "        # Standart  deviations\n",
    "        total_auc_rf_std = round(np.std(total_auc_rf),8)\n",
    "        total_f1_rf_std = round(np.std(total_f1_rf),8)\n",
    "\n",
    "        total_auc_lr_std = round(np.std(total_auc_lr),8)\n",
    "        total_f1_lr_std = round(np.std(total_f1_lr),8)\n",
    "\n",
    "        c = len(X_fit)\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = pd.DataFrame()\n",
    "        model_perf_metrics_mean_of_iterations = pd.DataFrame()\n",
    "\n",
    "        default_RF_means_row = {'Model': 'RF_default_mean_of_iterations', 'Iteration': k+1, 'Sample Size': 0, 'CF Num': 0, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_rf_mean, 'F1 (%)_std':total_f1_rf_std, 'AUC (%)' :total_auc_rf_mean, 'AUC (%)_std': total_auc_rf_std}\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_RF_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_RF_means_row, ignore_index=True)   \n",
    "\n",
    "       \n",
    "        default_LR_means_row = {'Model': 'DT_default_mean_of_iterations', 'Iteration': k+1, 'Sample Size': 0, 'CF Num': 0, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_lr_mean, 'F1 (%)_std':total_f1_lr_std, 'AUC (%)' :total_auc_lr_mean, 'AUC (%)_std': total_auc_lr_std}\n",
    "\n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_LR_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_LR_means_row, ignore_index=True)\n",
    "    \n",
    "        return model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the means and standart deviations of multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of model performance metrics for each iterations and constructs summary rows for inclusion in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    total_auc_cf (list): AUC scores for models trained only on counterfactuals across iterations.\n",
    "    total_f1_cf (list): F1 scores for models trained only on counterfactuals across iterations.\n",
    "    total_auc (list): AUC scores for models trained on original data plus counterfactuals across iterations.\n",
    "    total_f1 (list): F1 scores for models trained on original data plus counterfactuals across iterations.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    a (int): The size of the training dataset including original data plus counterfactuals.\n",
    "    b (int): The size of the training dataset with only counterfactuals.\n",
    "\n",
    "    Returns:\n",
    "    new_row (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained on original data plus counterfactuals.\n",
    "    new_row_cf (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained only on counterfactuals.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Calculate means of the performance metrics for models trained only on counterfactuals\n",
    "    total_auc_only_cf_mean = np.mean(total_auc_cf)\n",
    "    total_f1_only_cf_mean = np.mean(total_f1_cf)\n",
    "\n",
    "    # Calculate means of the performance metrics for models trained on original data plus counterfactuals\n",
    "    total_auc_mean = np.mean(total_auc)\n",
    "    total_f1_mean = np.mean(total_f1)\n",
    "\n",
    "    # Calculate standard deviations of the performance metrics for models trained only on counterfactuals\n",
    "    total_auc_only_cf_std = round(np.std(total_auc_cf), 8)\n",
    "    total_f1_only_cf_std = round(np.std(total_f1_cf), 8)\n",
    "\n",
    "    # Calculate standard deviations of the performance metrics for models trained on original data plus counterfactuals\n",
    "    total_auc_std = round(np.std(total_auc), 8)\n",
    "    total_f1_std = round(np.std(total_f1), 8)\n",
    "    \n",
    "    # Create a dictionary to store the performance metrics for models trained on original data plus counterfactuals    \n",
    "    new_row   = {'Model': 'DT_sample:{}_cf:{}_mean_of_{}_iterations'.format(j, i, k), 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':a ,\n",
    "                             'F1 (%)': total_f1_mean, 'F1 (%)_std':total_f1_std, 'AUC (%)' :total_auc_mean, 'AUC (%)_std': total_auc_std}\n",
    " \n",
    "                # Creating new rows to save the model performance metrics of each sample size and CF number combination's all iterations\n",
    "    new_row_cf= {'Model': 'DT_sample:{}_cf:{}_mean_of_{}_iterations_only_cf'.format(j, i, k) , 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':b , \n",
    "                             'F1 (%)': total_f1_only_cf_mean, 'F1 (%)_std': total_f1_only_cf_std, 'AUC (%)' :total_auc_only_cf_mean, 'AUC (%)_std': total_auc_only_cf_std}\n",
    "    return new_row, new_row_cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrubution_plot(df, name, true_false_1, true_false_2):\n",
    "    \"\"\"\n",
    "    This function plots the distribution of scores for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the model performance data.\n",
    "    name (str): The list of the Scores that will be plotted.\n",
    "    1st True False is a flag to determine if the data is mean of iterations or not.\n",
    "    2nd True False is a flag to determine if the data is only CFs or CFs + original data.    \n",
    "    \"\"\"\n",
    "    # Get the default values of the models\n",
    "    lr_line = df[df['Model'].str.startswith('DT_default')][name].iloc[0]\n",
    "    rf_line = df[df['Model'].str.startswith('RF_default')][name].iloc[0]\n",
    "\n",
    "    df = df[(df['Model'] != 'RF_default') & (df['Model'] != 'DT_default')& (df['Model'] != 'RF_simp')]\n",
    "    df = df[(df['Model'] != 'RF_default_mean_of_iterations') & (df['Model'] != 'DT_default_mean_of_iterations') & (df['Model'] != 'RF_simp_mean_of_iterations')]\n",
    "\n",
    "    # Get the sample size, CF Num and Iteration values    \n",
    "    x = df['Sample Size'].unique()\n",
    "    x_2 = df['CF Num'].unique()\n",
    "    x_3 = df['Iteration'].unique()\n",
    "\n",
    "    # Get the y values\n",
    "    y = df[name]\n",
    "    y_err = df[name+'_std']\n",
    "    # combine x and x_2 for x-axis labels\n",
    "    combined_x = [f\"{i}_{j}_{k}\" for i in x for j in x_2 for k in x_3]\n",
    "\n",
    "    # plot lines\n",
    "    plt.figure(figsize=(25,10))\n",
    "\n",
    "    \n",
    "    # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "    # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs\n",
    "    if true_false_1 is True:\n",
    "        plt.scatter(combined_x, y, color=\"blue\")\n",
    "        plt.errorbar(combined_x, y, yerr=y_err, fmt='o', color=\"blue\",  capsize=6, label=\"{} Values\".format(name))\n",
    "\n",
    "        plt.axhline(y=rf_line, color='r', linestyle='--', label=\"Default_RF_Line_Mean_of_{}\".format(x_3))  # Plot RF_line as a horizontal line\n",
    "        plt.axhline(y=lr_line, color='green', linestyle='--', label=\"Default_DT_line_Mean_of_{}\".format(x_3))  # Plot LR_line as a horizontal line\n",
    "\n",
    "        plt.ylabel('{} score'.format(name))\n",
    "        plt.xlabel('Sample Size- CF Num- Iteration')\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.05, 1.155))\n",
    "        plt.xticks(rotation=40)  # rotate x-axis labels for better visibility        \n",
    "        \n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models\\n Trained on CFs\\nSample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "        else:\n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "    else:\n",
    "        plt.plot(combined_x, y, 'o-', color=\"blue\", label=\"{} Values\".format(name))\n",
    "        \n",
    "        plt.axhline(y=rf_line, color='r', linestyle='--', label=\"Default_RF_Line\")  # Plot RF_line as a horizontal line\n",
    "        plt.axhline(y=lr_line, color='green', linestyle='--', label=\"Default_DT_line\")  # Plot LR_line as a horizontal line\n",
    "\n",
    "        plt.ylabel('{} score'.format(name))\n",
    "        plt.xlabel('Sample Size- CF Num- Iteration')\n",
    "        plt.legend(loc='upper right', bbox_to_anchor=(1.05, 1.155))\n",
    "        plt.xticks(rotation=40)  # rotate x-axis labels for better visibility    \n",
    "\n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models \\nTrained on CFs\\nSample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "        else:    \n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "            \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the DICE model. Possible adjustments that can be made are, \n",
    "- which model to use for CF generation, \n",
    "- continuous features, \n",
    "- size of the sample that will be used to create the CFs, \n",
    "- how many CFs to generate per one sample,\n",
    "- which features can vary in the CF generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DICE model\n",
    "def generate_counterfactuals(X_fit, y_fit, model, sample_size, total_CFs, fea_to_vary, k, continuous_features ):\n",
    "    \"\"\"\n",
    "    Generates counterfactual examples using the DICE (Diverse Counterfactual Explanations) method.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    model (object): The machine learning model for which counterfactuals are to be generated.\n",
    "    sample_size (int): The number of samples for which to generate counterfactuals.\n",
    "    total_CFs (int): The total number of counterfactuals to generate per sample.\n",
    "    fea_to_vary (list): The list of features to be varied in the counterfactual generation process.\n",
    "    outcome_name (str): The name of the outcome variable.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    X_fit_cf (pandas.DataFrame): The feature set of the generated counterfactuals.\n",
    "    y_fit_cf (pandas.Series): The target variable of the generated counterfactuals.\n",
    "    cf_df (pandas.DataFrame): A dataframe containing all the generated counterfactuals.\n",
    "    \"\"\"\n",
    "    all_columns = X_fit.columns\n",
    "    # Calculate the original class distribution\n",
    "    class_0_count = np.sum(y_fit == 0)\n",
    "    class_1_count = np.sum(y_fit == 1)\n",
    "\n",
    "    # Calculate the opposite class distribution proportions\n",
    "    total_count = class_0_count + class_1_count\n",
    "    prop_class_0 = class_1_count / total_count\n",
    "    prop_class_1 = class_0_count / total_count\n",
    "\n",
    "    # Determine the number of samples for each class in the opposite distribution\n",
    "    total_samples = sample_size\n",
    "    num_class_0_samples = int(total_samples * prop_class_0)\n",
    "    num_class_1_samples = total_samples - num_class_0_samples  # Ensuring total is 1000\n",
    "\n",
    "    # Sample from each class to get the opposite distribution\n",
    "    X_class_0 = X_fit[y_fit == 0]\n",
    "    X_class_1 = X_fit[y_fit == 1]\n",
    "\n",
    "    y_class_0 = y_fit[y_fit == 0]\n",
    "    y_class_1 = y_fit[y_fit == 1]\n",
    "\n",
    "    # Sample the required number of samples from each class\n",
    "    X_class_0_sampled, _, y_class_0_sampled, _ = train_test_split(X_class_0, y_class_0, train_size=num_class_0_samples, random_state=42)\n",
    "    X_class_1_sampled, _, y_class_1_sampled, _ = train_test_split(X_class_1, y_class_1, train_size=num_class_1_samples, random_state=42)\n",
    "    # Convert the sampled numpy arrays back to DataFrames\n",
    "    X_class_0_sampled_df = pd.DataFrame(X_class_0_sampled, columns=all_columns)\n",
    "    X_class_1_sampled_df = pd.DataFrame(X_class_1_sampled, columns=all_columns)\n",
    "\n",
    "    # Concatenate DataFrames\n",
    "    X_fit = pd.concat([X_class_0_sampled_df, X_class_1_sampled_df], ignore_index=True)\n",
    "    # Assuming y_class_0_sampled, y_class_1_sampled are numpy arrays with single column\n",
    "    y_fit = pd.DataFrame(np.concatenate([y_class_0_sampled, y_class_1_sampled]), columns=[outcome_name])\n",
    "    \n",
    "    # Optionally, shuffle the dataset to mix the classes\n",
    "    X_fit, y_fit = shuffle(X_fit, y_fit, random_state=42)\n",
    "\n",
    "    # Create a Dice-ML data object\n",
    "    d = dice_ml.Data(dataframe=X_fit.assign(income=y_fit), continuous_features=continuous_features, outcome_name=outcome_name)\n",
    "    # Creating a Model object\n",
    "    m = dice_ml.Model(model=model, backend=\"sklearn\")\n",
    "    # Creating the DICE model with data and model object \n",
    "    exp = dice_ml.Dice(d, m, method=\"random\")\n",
    "\n",
    "    # Execution of the DICE model to generate counterfactuals\n",
    "    e1 = exp.generate_counterfactuals(X_fit, total_CFs=total_CFs, desired_class=\"opposite\", features_to_vary= fea_to_vary)\n",
    "    \n",
    "    # Comment out the below line as CFs needed to be visualized in the notebook\n",
    "    #e1.visualize_as_dataframe(show_only_changes=True)\n",
    "\n",
    "    # Creating a new empty dataframe to store the generated CFs\n",
    "    cf_df = pd.DataFrame()\n",
    "    # For loop to append the counterfactuals of each sample, to a dataframe\n",
    "    for i in range(0, sample_size):\n",
    "        # Saving CFs\n",
    "        xd = e1.cf_examples_list[i].final_cfs_df\n",
    "        # Adding the generated CFs to the empty dataframe\n",
    "        cf_df = pd.concat([cf_df, xd])\n",
    "# If the sample size is reached and the CFs are generated for all the samples, followiing code resets the index of the dataframe and saves the generated CFs as a csv file.\n",
    "    else:\n",
    "        # Resetting the index of the dataframe to not have duplicate index number\n",
    "        cf_df.reset_index(drop=True, inplace=True)\n",
    "        new_start_index = len(X_fit) + i + len(cf_df)\n",
    "        cf_df.index += new_start_index \n",
    "        # Saving the generated CFs as a csv file\n",
    "        cf_df.to_csv('Generated_CFs/cf_df_{}_{}_{}.csv'.format(sample_size, total_CFs,k ))\n",
    "        \n",
    "        # Following code creates X and y fit sets for the generated CFs and returns them for further use.\n",
    "        X_fit_cf = cf_df.drop([outcome_name], axis=1)\n",
    "        y_fit_cf = cf_df[outcome_name]\n",
    "\n",
    "    return X_fit_cf, y_fit_cf, cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a experiment  fuction that runs the RF and LR, DICE model that generates counterfactuals, and then using those CFs for more model traning. \n",
    "#### Then the function saves these CF explanations into a dataframe and trains the LR again with the generated CFs. At the same time, model appends the original fit-set to CF dataset and trains the LR again with the combined version of the datasets. For each model training, LR model function appends the new model performance metrics into a dataframe. Possible adjustments that can be made are, \n",
    "- continuous features, \n",
    "- size of the sample that will be used to create the CFs, \n",
    "- how many CFs to generate per one sample,\n",
    "- how many iterations will be made for each sample_size and CF_number pair,\n",
    "- which features can vary in the CF generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment running function\n",
    "def running_exp (X_fit, y_fit, X_test, y_test, sample_size_list, total_CFs_list, iteration_num, fea_to_vary, name_of_the_exp, list_of_models, target_variable):\n",
    "    \"\"\"\n",
    "    This function is for running the experiments with different sample sizes, CF numbers and iterations.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    continuous_features (list): The list of continuous features in the dataset.\n",
    "    sample_size_list (list): The list of sample sizes to be used in the experiments.\n",
    "    total_CFs_list (list): The list of total CF numbers to be generated for each sample size.\n",
    "    iteration_num (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    fea_to_vary (list): The list of features to be varied in the CF generation process. Other features will be kept constant.\n",
    "    num (int): The number to be added to the file names to distinguish the results of different experiments.\n",
    "    list_of_models (list): The names of the Scores to be plotted. For example, AUC or F1.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_merged_with_for_cfs (pandas.DataFrame): The model performance metrics of the experiments trained with only CFs.\n",
    "    model_perf_metrics_merged (pandas.DataFrame): The model performance metrics of the experiments trained with CFs and original data.\n",
    "    new_cf_df (pandas.DataFrame): All generated CFs in every CF number, sample size and iterations.\n",
    "    \"\"\"\n",
    "    new_cf_df = pd.DataFrame()\n",
    "\n",
    "    # Running the default Random Forest and Logistic Regression models once and merging their performance metrics in a dataframe.\n",
    "    model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs = run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test)\n",
    "    \n",
    "    # Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations = mean_of_default_models (iteration_num)\n",
    "    \n",
    "    # Saving these new rows to the model performance metrics dataframes    \n",
    "    # Loop through different sample sizes\n",
    "    for j in sample_size_list:\n",
    "        # Loop through different total CF numbers\n",
    "        for i in total_CFs_list:\n",
    "            # Initializing empty lists for saving auc and f1 scores for each iteration\n",
    "            total_f1_cf = []\n",
    "            total_f1 = []\n",
    "            total_auc_cf = []\n",
    "            total_auc = []\n",
    "\n",
    "            # Loop through iteration numbers, doing a new experiment for each sample_size and CF_number combination\n",
    "            for k in range(1,iteration_num+1):    \n",
    "                # Generating counterfactuals using the 'generate_counterfactuals' function\n",
    "                X_fit_cf, y_fit_cf, cf_df = generate_counterfactuals(X_fit, y_fit, model_RF, j, i, fea_to_vary,  k, continuous_features)\n",
    "                \n",
    "                # Concatenating the original dataset with the generated CFs dataset\n",
    "                new_cf_df = new_cf_df.append(cf_df)\n",
    "                new_X_fit = pd.concat([X_fit, X_fit_cf])\n",
    "                new_y_fit = pd.concat([y_fit, y_fit_cf])\n",
    "\n",
    "                # Running the Logistic Regression model on the CFs dataset and the dataset with original X_fit plus CFs.\n",
    "                model_perf_metric_lr_only_cf, auc_only_cf, f1_only_cf = run_DT(X_fit_cf, y_fit_cf, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                model_perf_metric_lr_cf_org, auc_lr_cf, f1_lr_cf = run_DT(new_X_fit, new_y_fit, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)                \n",
    "             \n",
    "                # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "                total_f1_cf.append(f1_only_cf)\n",
    "                total_f1.append(f1_lr_cf)\n",
    "                total_auc_cf.append(auc_only_cf)\n",
    "                total_auc.append(auc_lr_cf)\n",
    "\n",
    "                # Saving the model performance metrics of each iteration\n",
    "                model_perf_metrics_merged_only_with_cfs = model_perf_metrics_merged_only_with_cfs.append(model_perf_metric_lr_only_cf, ignore_index=True)\n",
    "                model_perf_metrics_merged = model_perf_metrics_merged.append(model_perf_metric_lr_cf_org, ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "            # Calculate means and standard deviations of auc and f1 scores of all the iterations for each sample size and CF number combination\n",
    "                a = len(new_X_fit)\n",
    "                b = len(X_fit_cf)\n",
    "\n",
    "                new_row, new_row_cf = calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b)\n",
    "            \n",
    "                # Saving these new rows to the model performance metrics dataframes    \n",
    "                model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(new_row_cf, ignore_index=True)\n",
    "                model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(new_row, ignore_index=True)\n",
    "                \n",
    "    # Saving the model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_merged_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_merged_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_merged.to_excel('Performance_metrics/model_perf_metrics_merged_{}.xlsx'.format(name_of_the_exp))\n",
    "    # Saving the MEAN model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_mean_of_iterations.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_{}.xlsx'.format(name_of_the_exp))\n",
    "    \n",
    "    for k in list_of_models:\n",
    "        # k is the list of the Scores to be plotted\n",
    "        # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "        # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs'''\n",
    "        distrubution_plot(model_perf_metrics_merged, k, False, False )\n",
    "        distrubution_plot(model_perf_metrics_merged_only_with_cfs, k, False, True )\n",
    "\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations, k, True , False)\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations_only_with_cfs, k, True, True )\n",
    "        \n",
    "    return  model_perf_metrics_merged_only_with_cfs, model_perf_metrics_merged, new_cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the columns' categories\n",
    "numerical = ['age' ,'fnlwgt','capital-gain','capital-loss','hours-per-week']\n",
    "categorical = ['workclass','education-num','marital-status',  'relationship', 'race', 'sex']\n",
    "target_variable = 'income'\n",
    "outcome_name = 'income'\n",
    "continuous_features = ['capital-gain', 'capital-loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_to_vary = 'all'\n",
    "sample_size_list = [3166, 6331, 9495, 12000]\n",
    "total_CFs_list = [1,2]\n",
    "iteration_num = 5\n",
    "list_of_plots = ['AUC (%)'] \n",
    "model_perf_metrics_merged_only_with_cfs, model_perf_metrics_merged, neww = running_exp(X_fit, y_fit, X_test, y_test,  sample_size_list, total_CFs_list, iteration_num, fea_to_vary, 'Results ', list_of_plots, target_variable)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
