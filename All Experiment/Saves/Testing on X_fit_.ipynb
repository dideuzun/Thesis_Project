{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing LR and RF on test and fit datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score,  recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe from a csv file\n",
    "df = pd.read_csv(r'C:\\Users\\dideu\\OneDrive\\Documents\\DDB\\thesis\\Thesis_Project\\AdultIncome\\CensusAdultIncome.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Target value has values with dots, therefore 4 catagories\n",
    "df['income'] = df['income'].str.replace('.', '')\n",
    "\n",
    "# Replace the values in the target column with string '0' and '1'.\n",
    "df['income'] = df['income'].str.replace('<=50K', '0')  \n",
    "df['income'] = df['income'].str.replace('>50K', '1')   \n",
    "\n",
    "# Convert to integer\n",
    "df['income'] = df['income'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some values are like '?'. Replacing them as NaN \n",
    "df[df == '?'] = np.nan\n",
    "# Dropping the rows with NaN values in  'workclass', 'occupation', 'native-country' for the test dataset\n",
    "df.dropna(subset=['workclass', 'occupation', 'native-country'], inplace=True)\n",
    "df.dropna( inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['education'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y sets.income is the target variable and native country dropped because it has 41 unique values and occupation is very similar too workclass.\n",
    "X = df.drop(['income', 'native-country', 'occupation'], axis=1)\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit, X_test, y_fit, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Random Forest model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_RF (X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "    \"\"\"\n",
    "    Trains a Random Forest model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    model_RF (Pipeline): The trained Random Forest model pipeline.\n",
    "    model_perf_metrics_rf (pandas.DataFrame): The performance metrics of the Random Forest model.\n",
    "    roc_rf (float): The ROC AUC score of the Random Forest model.\n",
    "    f1_rf (float): The F1 score of the Random Forest model.\n",
    "    \"\"\"    \n",
    "    # Preprocessing for cataegorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Bundle preprocessing for data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('cat', categorical_transformer, categorical)],\n",
    "        remainder='passthrough')\n",
    "    \n",
    "    # Define the Random Forest model\n",
    "    clf_RF = Pipeline(steps=[('preprocessor', transformations), ('classifier', RandomForestClassifier())])\n",
    "    # Run the random forest model\n",
    "    model_RF = clf_RF.fit(X_fit, y_fit)\n",
    "    # Predict on the test dataset\n",
    "    y_pred_rf = model_RF.predict(X_test)\n",
    "    y_pred_proba_rf = model_RF.predict_proba(X_test)\n",
    "    # Calculate the needed model performance metrics\n",
    "    f1_rf = f1_score(y_test, y_pred_rf)\n",
    "    roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "    acc = accuracy_score(y_test, y_pred_rf)\n",
    "    auc_sec = roc_auc_score(y_test, y_pred_proba_rf[:,1])\n",
    "    a = len(X_fit)\n",
    "    # Saving the model performance metrics\n",
    "    models_rf = [('RF_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metrics_rf = pd.DataFrame(models_rf, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size',  'F1 (%)', 'AUC (%)', 'F1_std(%)', 'AUC_std(%)'])\n",
    "        \n",
    "    return model_RF, model_perf_metrics_rf, roc_rf , f1_rf , acc,  auc_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Logistic Regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LR(X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "    \"\"\"\n",
    "    Trains a Logistic Regression model and evaluates its performance on the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    model_name (str): A name for the model to distinguish it in the performance metrics.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metric_lr (pandas.DataFrame): The performance metrics of the Logistic Regression model.\n",
    "    roc_lr (float): The ROC AUC score of the Logistic Regression model.\n",
    "    f1_lr (float): The F1 score of the Logistic Regression model.\n",
    "    \"\"\"    \n",
    "    # Preprocessing for categorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Preprocessing for numerical data - StandardScaler\n",
    "    #numerical_transformer = Pipeline(steps=[\n",
    "    #    ('scaler', StandardScaler())])\n",
    "\n",
    "    # Bundle preprocessing for numerical and categorical data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "        #    ('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)], \n",
    "\n",
    "        remainder='passthrough')\n",
    "\n",
    "    # Define the Logistic Regression model\n",
    "    clf_LR = Pipeline(steps=[('preprocessor', transformations), ('classifier', LogisticRegression(max_iter=1000))])\n",
    "    # Run the Logistic Regression model\n",
    "    model_LR = clf_LR.fit(X_fit, y_fit)\n",
    "    # Predict on the test dataset\n",
    "    y_pred_lr = model_LR.predict(X_test)\n",
    "    y_pred_proba_lr = model_LR.predict_proba(X_test)\n",
    "    # Calculate the needed model performance metrics\n",
    "    f1_lr = f1_score(y_test, y_pred_lr)\n",
    "    roc_lr = roc_auc_score(y_test, y_pred_lr)\n",
    "    acc = accuracy_score(y_test, y_pred_lr)\n",
    "    auc_sec = roc_auc_score(y_test, y_pred_proba_lr[:,1])\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Saving the model performance metrics\n",
    "    models_lr = [('LR_{}'.format(model_name), k, j, i, a, f1_lr, roc_lr, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metric_lr = pd.DataFrame(models_lr, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num','X_fit Size', 'F1 (%)', 'AUC (%)', 'F1_std(%)', 'AUC_std(%)'])\n",
    "\n",
    "    return  model_perf_metric_lr, roc_lr, f1_lr, acc, auc_sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models ONE TIME and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Trains Random Forest and Logistic Regression models on the given training data,evaluates their performance on the test data, and merges their performance metrics into two dataframes.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (DataFrame): Features for training the models.\n",
    "    y_fit (Series): Target variable for training the models.\n",
    "    X_test (DataFrame): Features for testing the models.\n",
    "    y_test (Series): Target variable for testing the models.\n",
    "\n",
    "    Returns:\n",
    "    tuple: \n",
    "        - model_RF (RandomForestClassifier): The trained Random Forest model.\n",
    "        - model_perf_metrics_merged (DataFrame): Dataframe containing merged performance metrics of the Random Forest and Logistic Regression models.\n",
    "        - model_perf_metrics_merged_only_with_cfs (DataFrame): Another dataframe containing merged performance metrics, identical to the first but intended for use with a counterfactuals dataset.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Running default Random Forest Once\n",
    "    model_RF, model_perf_metric_rf, roc_rf, f1_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0)\n",
    "    # Running default Logistic Regression Once\n",
    "    model_perf_metric_lr, auc, f1 = run_LR(X_fit, y_fit, X_test, y_test, 'default', 0, 0, 0)\n",
    "    \n",
    "    # Merging the default model performance metrics in a dataframe to accommodate new models, and doing it twice to use the second one on only CFs dataset.\n",
    "    model_perf_metrics_merged = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    model_perf_metrics_merged_only_with_cfs = model_perf_metric_rf.merge(model_perf_metric_lr, how='outer')\n",
    "    \n",
    "    return model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the default models MULTIPLE TIMES and saving the performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_of_default_models (k, X_fit, y_fit, X_test, y_test, name):\n",
    "    \"\"\"\n",
    "    Computes the mean and standard deviation of model performance metrics for default Random Forest and Logistic Regression models over multiple iterations.\n",
    "\n",
    "    Parameters:\n",
    "    k (int): The number of iterations to run the models for calculating the mean and standard deviation.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained only on CFs.\n",
    "    model_perf_metrics_mean_of_iterations (pandas.DataFrame): DataFrame containing the mean and standard deviation of performance metrics for Random Forest and Logistic Regression models trained on original data.\n",
    "    \"\"\"\n",
    "    total_auc_rf = []\n",
    "    total_f1_rf = []\n",
    "    total_auc_lr = []\n",
    "    total_f1_lr = []\n",
    "    \n",
    "    for k in range(0, k):    \n",
    "        #Running default RF and LR\n",
    "        _, _ ,auc_def_rf, f1_def_rf, acc_rf, auc_se_rf = run_RF(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0)\n",
    "        _ , auc_def_lr, f1_def_lr, acc_lr, auc_se_lr = run_LR(X_fit, y_fit, X_test, y_test, 'default', 0 , 0 , 0)\n",
    "        \n",
    "        # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "        total_auc_rf.append(auc_def_rf)\n",
    "        total_auc_lr.append(auc_def_lr)\n",
    "        total_f1_rf.append(f1_def_rf)\n",
    "        total_f1_lr.append(f1_def_lr)\n",
    "    else:\n",
    "        # Means            \n",
    "        total_auc_rf_mean = np.mean(total_auc_rf)\n",
    "        total_f1_rf_mean = np.mean(total_f1_rf)\n",
    "\n",
    "        total_auc_lr_mean = np.mean(total_auc_lr)\n",
    "        total_f1_lr_mean = np.mean(total_f1_lr)\n",
    "\n",
    "        # Standart  deviations\n",
    "        total_auc_rf_std = round(np.std(total_auc_rf),8)\n",
    "        total_f1_rf_std = round(np.std(total_f1_rf),8)\n",
    "\n",
    "        total_auc_lr_std = round(np.std(total_auc_lr),8)\n",
    "        total_f1_lr_std = round(np.std(total_f1_lr),8)\n",
    "\n",
    "        c = len(X_fit)\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = pd.DataFrame()\n",
    "        model_perf_metrics_mean_of_iterations = pd.DataFrame()\n",
    "\n",
    "        default_RF_means_row = {'Model': 'RF_default_mean_of_iterations tested on: {}'.format(name), 'Iteration': k+1, 'Sample Size': 0, 'CF Num': auc_se_rf, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_rf_mean, 'F1_std(%)':total_f1_rf_std, 'AUC (%)' :total_auc_rf_mean, 'AUC_std(%)': total_auc_rf_std}\n",
    "        \n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_RF_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_RF_means_row, ignore_index=True)   \n",
    "       \n",
    "        default_LR_means_row = {'Model': 'LR_default_mean_of_iterations tested on: {}'.format(name), 'Iteration': k+1, 'Sample Size': 0, 'CF Num': auc_se_lr, 'X_fit Size':c ,\n",
    "        'F1 (%)': total_f1_lr_mean, 'F1_std(%)':total_f1_lr_std, 'AUC (%)' :total_auc_lr_mean, 'AUC_std(%)': total_auc_lr_std}\n",
    "\n",
    "        model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(default_LR_means_row, ignore_index=True)\n",
    "        model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(default_LR_means_row, ignore_index=True)\n",
    "    \n",
    "        return model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the means and standart deviations of multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b):\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of model performance metrics for each iterations and constructs summary rows for inclusion in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    total_auc_cf (list): AUC scores for models trained only on counterfactuals across iterations.\n",
    "    total_f1_cf (list): F1 scores for models trained only on counterfactuals across iterations.\n",
    "    total_auc (list): AUC scores for models trained on original data plus counterfactuals across iterations.\n",
    "    total_f1 (list): F1 scores for models trained on original data plus counterfactuals across iterations.\n",
    "    i (int): The number of counterfactuals to be generated for each sample, for the experiment.\n",
    "    j (int): The sample size to be used for counterfactual generation, for the experiment.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    a (int): The size of the training dataset including original data plus counterfactuals.\n",
    "    b (int): The size of the training dataset with only counterfactuals.\n",
    "\n",
    "    Returns:\n",
    "    new_row (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained on original data plus counterfactuals.\n",
    "    new_row_cf (dict): A dictionary containing the mean and standard deviation of performance metrics for models trained only on counterfactuals.\n",
    "    \"\"\"\n",
    "        \n",
    "    total_auc_only_cf_mean = np.mean(total_auc_cf)\n",
    "    total_f1_only_cf_mean = np.mean(total_f1_cf)\n",
    "\n",
    "    total_auc_mean = np.mean(total_auc)\n",
    "    total_f1_mean = np.mean(total_f1)\n",
    "\n",
    "    #Standart  deviations\n",
    "    total_auc_only_cf_std = round(np.std(total_auc_cf),8)\n",
    "    total_f1_only_cf_std = round(np.std(total_f1_cf),8)\n",
    "\n",
    "    total_auc_std = round(np.std(total_auc),8)\n",
    "    total_f1_std = round(np.std(total_f1),8)\n",
    "    \n",
    "    new_row   = {'Model': 'LR_sample:{}_cf:{}_mean_of_{}_iterations'.format(j, i, k), 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':a ,\n",
    "                             'F1 (%)': total_f1_mean, 'F1_std(%)':total_f1_std, 'AUC (%)' :total_auc_mean, 'AUC_std(%)': total_auc_std}\n",
    " \n",
    "                # Creating new rows to save the model performance metrics of each sample size and CF number combination's all iterations\n",
    "    new_row_cf= {'Model': 'LR_sample:{}_cf:{}_mean_of_{}_iterations_only_cf'.format(j, i, k) , 'Iteration': k, 'Sample Size': j, 'CF Num': i, 'X_fit Size':b , \n",
    "                             'F1 (%)': total_f1_only_cf_mean, 'F1_std(%)': total_f1_only_cf_std, 'AUC (%)' :total_auc_only_cf_mean, 'AUC_std(%)': total_auc_only_cf_std}\n",
    "    return new_row, new_row_cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distrubution_plot(df, name, true_false_1, true_false_2):\n",
    "    \"\"\"\n",
    "    This function plots the distribution of scores for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the model performance data.\n",
    "    name (str): The list of the Scores that will be plotted.\n",
    "    1st True False is a flag to determine if the data is mean of iterations or not.\n",
    "    2nd True False is a flag to determine if the data is only CFs or CFs + original data.    \n",
    "    \"\"\"\n",
    "    # Get the default values of the models\n",
    "    lr_line = df[df['Model'].str.startswith('LR_default')][name].iloc[0]\n",
    "    rf_line = df[df['Model'].str.startswith('RF_default')][name].iloc[0]\n",
    "    df = df[(df['Model'] != 'RF_default') & (df['Model'] != 'LR_default')]\n",
    "    df = df[(df['Model'] != 'RF_default_mean_of_iterations') & (df['Model'] != 'LR_default_mean_of_iterations')]\n",
    "\n",
    "    # Get the sample size, CF Num and Iteration values    \n",
    "    x = df['Sample Size'].unique()\n",
    "    x_2 = df['CF Num'].unique()\n",
    "    x_3 = df['Iteration'].unique()\n",
    "\n",
    "    # Get the y values\n",
    "    y = df[name]\n",
    "    # combine x and x_2 for x-axis labels\n",
    "    combined_x = [f\"{i}_{j}_{k}\" for i in x for j in x_2 for k in x_3]\n",
    "\n",
    "    # plot lines\n",
    "    plt.figure(figsize=(25,10))\n",
    "    plt.plot(combined_x, y, 'o-', label=\"{} Values\".format(name))\n",
    "    plt.axhline(y=rf_line, color='r', linestyle='--', label=\"Default_RF_Line\")  # Plot RF_line as a horizontal line\n",
    "    plt.axhline(y=lr_line, color='green', linestyle='--', label=\"Default_LR_line\")  # Plot LR_line as a horizontal line\n",
    "\n",
    "    plt.ylabel('{} score'.format(name))\n",
    "    plt.xlabel('Sample Size- CF Num- Iteration')\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.05, 1.155))\n",
    "    plt.xticks(rotation=40)  # rotate x-axis labels for better visibility\n",
    "    \n",
    "    # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "    # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs\n",
    "    if true_false_1 is True:\n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models\\n Trained on CFs\\nSample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "        else:\n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}\\n Mean of {} iterations'.format(name,x, x_2, x_3))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}_mean_of_{x_3}_iterations.svg\", format='svg')    \n",
    "    else:\n",
    "        if true_false_2 is True:\n",
    "            plt.title('Distribution of {} Scores of Models \\nTrained on CFs\\nSample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_onlyCFs_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "        else:    \n",
    "            plt.title('Distribution of {} Scores \\n Trained on CFs + Original set\\n Sample: {} - CF: {}'.format(name, x, x_2))\n",
    "            plt.savefig(f\"Plots/Plot_of_{name}_Score_Sample_{x}_CF_{x_2}.svg\", format='svg')\n",
    "            \n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Counterfactuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining DICE model\n",
    "def generate_counterfactuals(X_fit, y_fit, model, continuous_features, sample_size, total_CFs, fea_to_vary, outcome_name, k ):\n",
    "    \"\"\"\n",
    "    Generates counterfactual examples using the DICE (Diverse Counterfactual Explanations) method.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    model (object): The machine learning model for which counterfactuals are to be generated.\n",
    "    continuous_features (list): The list of continuous features in the dataset.\n",
    "    sample_size (int): The number of samples for which to generate counterfactuals.\n",
    "    total_CFs (int): The total number of counterfactuals to generate per sample.\n",
    "    fea_to_vary (list): The list of features to be varied in the counterfactual generation process.\n",
    "    outcome_name (str): The name of the outcome variable.\n",
    "    k (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "\n",
    "    Returns:\n",
    "    X_fit_cf (pandas.DataFrame): The feature set of the generated counterfactuals.\n",
    "    y_fit_cf (pandas.Series): The target variable of the generated counterfactuals.\n",
    "    cf_df (pandas.DataFrame): A dataframe containing all the generated counterfactuals.\n",
    "    \"\"\"\n",
    "        \n",
    "    # Creating a Data object\n",
    "    d = dice_ml.Data(dataframe=X_fit.assign(income=y_fit), continuous_features=continuous_features, outcome_name=outcome_name)\n",
    "    # Creating a Model object\n",
    "    m = dice_ml.Model(model=model, backend=\"sklearn\")\n",
    "    # Creating the DICE model with data and model object \n",
    "    exp = dice_ml.Dice(d, m, method=\"random\")\n",
    "\n",
    "    # Execution of the DICE model to generate counterfactuals\n",
    "    e1 = exp.generate_counterfactuals(X_fit[0:sample_size], total_CFs=total_CFs, desired_class=\"opposite\", features_to_vary= fea_to_vary)\n",
    "    \n",
    "    # Comment out the below line as CFs needed to be visualized in the notebook\n",
    "    #e1.visualize_as_dataframe(show_only_changes=True)\n",
    "\n",
    "    # Creating a new empty dataframe to store the generated CFs\n",
    "    cf_df = pd.DataFrame()\n",
    "    # For loop to append the counterfactuals of each sample, to a dataframe\n",
    "    for i in range(0, sample_size):\n",
    "        # Saving CFs\n",
    "        xd = e1.cf_examples_list[i].final_cfs_df\n",
    "        # Adding the generated CFs to the empty dataframe\n",
    "        cf_df = pd.concat([cf_df, xd])\n",
    "# If the sample size is reached and the CFs are generated for all the samples, followiing code resets the index of the dataframe and saves the generated CFs as a csv file.\n",
    "    else:\n",
    "        # Resetting the index of the dataframe to not have duplicate index number\n",
    "        cf_df.reset_index(drop=True, inplace=True)\n",
    "        new_start_index = len(X_fit) + i + len(cf_df)\n",
    "        cf_df.index += new_start_index \n",
    "        # Saving the generated CFs as a csv file\n",
    "        cf_df.to_csv('Generated_CFs/cf_df_{}_{}_{}.csv'.format(sample_size, total_CFs,k ))\n",
    "        \n",
    "        # Following code creates X and y fit sets for the generated CFs and returns them for further use.\n",
    "        X_fit_cf = cf_df.drop([outcome_name], axis=1)\n",
    "        y_fit_cf = cf_df[outcome_name]\n",
    "\n",
    "    return X_fit_cf, y_fit_cf, cf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a experiment  fuction that runs the RF and LR, DICE model that generates counterfactuals, and then using those CFs for more model traning. \n",
    "#### Then the function saves these CF explanations into a dataframe and trains the LR again with the generated CFs. At the same time, model appends the original fit-set to CF dataset and trains the LR again with the combined version of the datasets. For each model training, LR model function appends the new model performance metrics into a dataframe. Possible adjustments that can be made are, \n",
    "- continuous features, \n",
    "- size of the sample that will be used to create the CFs, \n",
    "- how many CFs to generate per one sample,\n",
    "- how many iterations will be made for each sample_size and CF_number pair,\n",
    "- which features can vary in the CF generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment running function\n",
    "def running_exp (X_fit, y_fit, X_test, y_test, continuous_features, sample_size_list, total_CFs_list, iteration_num, fea_to_vary, name_of_the_exp, list_of_models):\n",
    "    \"\"\"\n",
    "    This function is for running the experiments with different sample sizes, CF numbers and iterations.\n",
    "\n",
    "    Parameters:\n",
    "    X_fit (pandas.DataFrame): The training dataset.\n",
    "    y_fit (pandas.Series): The target variable of the training dataset.\n",
    "    X_test (pandas.DataFrame): The test dataset.\n",
    "    y_test (pandas.Series): The target variable of the test dataset.\n",
    "    continuous_features (list): The list of continuous features in the dataset.\n",
    "    sample_size_list (list): The list of sample sizes to be used in the experiments.\n",
    "    total_CFs_list (list): The list of total CF numbers to be generated for each sample size.\n",
    "    iteration_num (int): The number of iterations to be run for each sample size and CF number combination.\n",
    "    fea_to_vary (list): The list of features to be varied in the CF generation process. Other features will be kept constant.\n",
    "    num (int): The number to be added to the file names to distinguish the results of different experiments.\n",
    "    list_of_models (list): The names of the Scores to be plotted. For example, AUC or F1.\n",
    "\n",
    "    Returns:\n",
    "    model_perf_metrics_merged_with_for_cfs (pandas.DataFrame): The model performance metrics of the experiments trained with only CFs.\n",
    "    model_perf_metrics_merged (pandas.DataFrame): The model performance metrics of the experiments trained with CFs and original data.\n",
    "    new_cf_df (pandas.DataFrame): All generated CFs in every CF number, sample size and iterations.\n",
    "    \"\"\"\n",
    "    new_cf_df = pd.DataFrame()\n",
    "\n",
    "    # Running the default Random Forest and Logistic Regression models ONCE and merging their performance metrics in a dataframe.\n",
    "    model_RF, model_perf_metrics_merged, model_perf_metrics_merged_only_with_cfs = run_models_and_merge_metrics(X_fit, y_fit, X_test, y_test)\n",
    "    \n",
    "    # Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs, model_perf_metrics_mean_of_iterations = mean_of_default_models (iteration_num)\n",
    "    \n",
    "    # Saving these new rows to the model performance metrics dataframes    \n",
    "    # Loop through different sample sizes\n",
    "    for j in sample_size_list:\n",
    "        # Loop through different total CF numbers\n",
    "        for i in total_CFs_list:\n",
    "            # Initializing empty lists for saving auc and f1 scores for each iteration\n",
    "            total_f1_cf = []\n",
    "            total_f1 = []\n",
    "            total_auc_cf = []\n",
    "            total_auc = []\n",
    "\n",
    "            # Loop through iteration numbers, doing a new experiment for each sample_size and CF_number combination\n",
    "            for k in range(1,iteration_num+1):    \n",
    "                # Generating counterfactuals using the 'generate_counterfactuals' function\n",
    "                X_fit_cf, y_fit_cf, cf_df = generate_counterfactuals(X_fit, y_fit, model_RF, continuous_features, j, i, fea_to_vary, 'income', k)\n",
    "                # Concatenating the original dataset with the generated CFs dataset\n",
    "                # \n",
    "                new_cf_df = new_cf_df.append(cf_df)\n",
    "                new_X_fit = pd.concat([X_fit, X_fit_cf])\n",
    "                new_y_fit = pd.concat([y_fit, y_fit_cf])\n",
    "\n",
    "                # Running the Logistic Regression model on the CFs dataset and the dataset with original X_fit plus CFs.\n",
    "                model_perf_metric_lr_only_cf, auc_only_cf, f1_only_cf = run_LR(X_fit_cf, y_fit_cf, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                model_perf_metric_lr_cf, auc_lr_cf, f1_lr_cf = run_LR(new_X_fit, new_y_fit, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)                \n",
    "                # Running the Random Forest model on the CFs dataset and the dataset with original X_fit plus CFs.\n",
    "                #_,model_perf_metric_lr_only_cf, auc_only_cf, f1_only_cf = run_RF(X_fit_cf, y_fit_cf, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                #_, model_perf_metric_lr_cf, auc_lr_cf, f1_lr_cf = run_RF(new_X_fit, new_y_fit, X_test, y_test, 'sample:{}_cf:{}_iteration:{}'.format(j, i, k), i , j , k)\n",
    "                # Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\n",
    "                total_f1_cf.append(f1_only_cf)\n",
    "                total_f1.append(f1_lr_cf)\n",
    "                total_auc_cf.append(auc_only_cf)\n",
    "                total_auc.append(auc_lr_cf)\n",
    "\n",
    "                # Saving the model performance metrics of each iteration\n",
    "                model_perf_metrics_merged_only_with_cfs = model_perf_metrics_merged_only_with_cfs.append(model_perf_metric_lr_only_cf, ignore_index=True)\n",
    "                model_perf_metrics_merged = model_perf_metrics_merged.append(model_perf_metric_lr_cf, ignore_index=True)\n",
    "                \n",
    "            else:\n",
    "            # Calculate means and standard deviations of auc and f1 scores of all the iterations for each sample size and CF number combination\n",
    "                a = len(new_X_fit)\n",
    "                b = len(X_fit_cf)\n",
    "\n",
    "                new_row, new_row_cf = calculate_means_and_stds(total_auc_cf, total_f1_cf, total_auc, total_f1, i, j, k, a, b)\n",
    "            \n",
    "                # Saving these new rows to the model performance metrics dataframes    \n",
    "                model_perf_metrics_mean_of_iterations_only_with_cfs = model_perf_metrics_mean_of_iterations_only_with_cfs.append(new_row_cf, ignore_index=True)\n",
    "                model_perf_metrics_mean_of_iterations = model_perf_metrics_mean_of_iterations.append(new_row, ignore_index=True)\n",
    "                \n",
    "    # Saving the model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_merged_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_merged_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_merged.to_excel('Performance_metrics/model_perf_metrics_merged_{}.xlsx'.format(name_of_the_exp))\n",
    "    # Saving the MEAN model performance metrics of each iteration as excel files after all the experiments are done.\n",
    "    model_perf_metrics_mean_of_iterations_only_with_cfs.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_only_with_cfs_{}.xlsx'.format(name_of_the_exp))\n",
    "    model_perf_metrics_mean_of_iterations.to_excel('Performance_metrics/model_perf_metrics_mean_of_iterations_{}.xlsx'.format(name_of_the_exp))\n",
    "    \n",
    "    for k in list_of_models:\n",
    "        # k is the list of the Scores to be plotted\n",
    "        # 1st True False is a flag to determine if the data is mean of iterations or not. True = mean of iterations\n",
    "        # 2nd True False is a flag to determine if the data is only CFs or CFs + original data. True = only CFs\n",
    "        distrubution_plot(model_perf_metrics_merged, k, False, False )\n",
    "        distrubution_plot(model_perf_metrics_merged_only_with_cfs, k, False, True )\n",
    "\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations, k, True , False)\n",
    "        distrubution_plot(model_perf_metrics_mean_of_iterations_only_with_cfs, k, True, True )\n",
    "        \n",
    "    return  model_perf_metrics_merged_only_with_cfs, model_perf_metrics_merged, new_cf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the columns' categories\n",
    "numerical = ['age' ,'fnlwgt','education-num','capital-gain','capital-loss','hours-per-week']\n",
    "categorical = ['workclass','marital-status',  'relationship', 'race', 'sex']\n",
    "continuous_features = ['capital-gain', 'capital-loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_num = 3\n",
    "# Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "_, model_perf_metrics_mean_of_iterations_on_X_test = mean_of_default_models (iteration_num, X_fit, y_fit, X_test, y_test, 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_num = 3\n",
    "# Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "_, model_perf_metrics_mean_of_iterations_on_X_fit = mean_of_default_models (iteration_num, X_fit, y_fit, X_fit, y_fit, 'X_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Sample Size</th>\n",
       "      <th>CF Num</th>\n",
       "      <th>X_fit Size</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>F1_std(%)</th>\n",
       "      <th>AUC (%)</th>\n",
       "      <th>AUC_std(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_default_mean_of_iterations tested on: X_test</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.891654</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.658000</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.765006</td>\n",
       "      <td>0.001009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR_default_mean_of_iterations tested on: X_test</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.579357</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.383472</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613885</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_default_mean_of_iterations tested on: X_fit</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.999354</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.999488</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR_default_mean_of_iterations tested on: X_fit</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.574626</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.382631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Iteration  Sample Size  \\\n",
       "0  RF_default_mean_of_iterations tested on: X_test          3            0   \n",
       "1  LR_default_mean_of_iterations tested on: X_test          3            0   \n",
       "2   RF_default_mean_of_iterations tested on: X_fit          3            0   \n",
       "3   LR_default_mean_of_iterations tested on: X_fit          3            0   \n",
       "\n",
       "     CF Num  X_fit Size    F1 (%)  F1_std(%)   AUC (%)  AUC_std(%)  \n",
       "0  0.891654       27133  0.658000   0.001308  0.765006    0.001009  \n",
       "1  0.579357       27133  0.383472   0.000000  0.613885    0.000000  \n",
       "2  1.000000       27133  0.999354   0.000035  0.999488    0.000031  \n",
       "3  0.574626       27133  0.382631   0.000000  0.613363    0.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf_metrics_merged = model_perf_metrics_mean_of_iterations_on_X_test.merge(model_perf_metrics_mean_of_iterations_on_X_fit, how='outer')\n",
    "model_perf_metrics_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Model</th>\n",
    "      <th>Iteration</th>\n",
    "      <th>Sample Size</th>\n",
    "      <th>CF Num</th>\n",
    "      <th>X_fit Size</th>\n",
    "      <th>F1 (%)</th>\n",
    "      <th>F1_std(%)</th>\n",
    "      <th>AUC (%)</th>\n",
    "      <th>AUC_std(%)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>RF_default_mean_of_iterations tested on: X_test</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.657382</td>\n",
    "      <td>0.002113</td>\n",
    "      <td>0.764474</td>\n",
    "      <td>0.001373</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>LR_default_mean_of_iterations tested on: X_test</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.647489</td>\n",
    "      <td>0.000000</td>\n",
    "      <td>0.755191</td>\n",
    "      <td>0.000000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>RF_default_mean_of_iterations tested on: X_fit</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.999322</td>\n",
    "      <td>0.000085</td>\n",
    "      <td>0.999427</td>\n",
    "      <td>0.000110</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>LR_default_mean_of_iterations tested on: X_fit</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.641738</td>\n",
    "      <td>0.000000</td>\n",
    "      <td>0.752036</td>\n",
    "      <td>0.000000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# South German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# DiCE imports\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers  # helper functions\n",
    "from sklearn.metrics import accuracy_score,  recall_score, precision_score, f1_score, roc_auc_score, roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>duration</th>\n",
       "      <th>credit_history</th>\n",
       "      <th>purpose</th>\n",
       "      <th>amount</th>\n",
       "      <th>savings</th>\n",
       "      <th>employment_duration</th>\n",
       "      <th>installment_rate</th>\n",
       "      <th>personal_status_sex</th>\n",
       "      <th>other_debtors</th>\n",
       "      <th>...</th>\n",
       "      <th>property</th>\n",
       "      <th>age</th>\n",
       "      <th>other_installment_plans</th>\n",
       "      <th>housing</th>\n",
       "      <th>number_credits</th>\n",
       "      <th>job</th>\n",
       "      <th>people_liable</th>\n",
       "      <th>telephone</th>\n",
       "      <th>foreign_worker</th>\n",
       "      <th>credit_risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>no checking account</td>\n",
       "      <td>18</td>\n",
       "      <td>all credits at this bank paid back duly</td>\n",
       "      <td>car (used)</td>\n",
       "      <td>1049</td>\n",
       "      <td>unknown/no savings account</td>\n",
       "      <td>&lt; 1 yr</td>\n",
       "      <td>&lt; 20</td>\n",
       "      <td>female : non-single or male : single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>car or other</td>\n",
       "      <td>21</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>1</td>\n",
       "      <td>skilled employee/official</td>\n",
       "      <td>0 to 2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no checking account</td>\n",
       "      <td>9</td>\n",
       "      <td>all credits at this bank paid back duly</td>\n",
       "      <td>others</td>\n",
       "      <td>2799</td>\n",
       "      <td>unknown/no savings account</td>\n",
       "      <td>1 &lt;= ... &lt; 4 yrs</td>\n",
       "      <td>25 &lt;= ... &lt; 35</td>\n",
       "      <td>male : married/widowed</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown / no property</td>\n",
       "      <td>36</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>3-Feb</td>\n",
       "      <td>skilled employee/official</td>\n",
       "      <td>3 or more</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... &lt; 0 DM</td>\n",
       "      <td>12</td>\n",
       "      <td>no credits taken/all credits paid back duly</td>\n",
       "      <td>retraining</td>\n",
       "      <td>841</td>\n",
       "      <td>... &lt;  100 DM</td>\n",
       "      <td>4 &lt;= ... &lt; 7 yrs</td>\n",
       "      <td>25 &lt;= ... &lt; 35</td>\n",
       "      <td>female : non-single or male : single</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown / no property</td>\n",
       "      <td>23</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>1</td>\n",
       "      <td>unskilled - resident</td>\n",
       "      <td>0 to 2</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no checking account</td>\n",
       "      <td>12</td>\n",
       "      <td>all credits at this bank paid back duly</td>\n",
       "      <td>others</td>\n",
       "      <td>2122</td>\n",
       "      <td>unknown/no savings account</td>\n",
       "      <td>1 &lt;= ... &lt; 4 yrs</td>\n",
       "      <td>20 &lt;= ... &lt; 25</td>\n",
       "      <td>male : married/widowed</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown / no property</td>\n",
       "      <td>39</td>\n",
       "      <td>none</td>\n",
       "      <td>for free</td>\n",
       "      <td>3-Feb</td>\n",
       "      <td>unskilled - resident</td>\n",
       "      <td>3 or more</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no checking account</td>\n",
       "      <td>12</td>\n",
       "      <td>all credits at this bank paid back duly</td>\n",
       "      <td>others</td>\n",
       "      <td>2171</td>\n",
       "      <td>unknown/no savings account</td>\n",
       "      <td>1 &lt;= ... &lt; 4 yrs</td>\n",
       "      <td>&lt; 20</td>\n",
       "      <td>male : married/widowed</td>\n",
       "      <td>none</td>\n",
       "      <td>...</td>\n",
       "      <td>car or other</td>\n",
       "      <td>38</td>\n",
       "      <td>bank</td>\n",
       "      <td>rent</td>\n",
       "      <td>3-Feb</td>\n",
       "      <td>unskilled - resident</td>\n",
       "      <td>0 to 2</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                status  duration                               credit_history  \\\n",
       "0  no checking account        18      all credits at this bank paid back duly   \n",
       "1  no checking account         9      all credits at this bank paid back duly   \n",
       "2           ... < 0 DM        12  no credits taken/all credits paid back duly   \n",
       "3  no checking account        12      all credits at this bank paid back duly   \n",
       "4  no checking account        12      all credits at this bank paid back duly   \n",
       "\n",
       "      purpose  amount                     savings employment_duration  \\\n",
       "0  car (used)    1049  unknown/no savings account              < 1 yr   \n",
       "1      others    2799  unknown/no savings account    1 <= ... < 4 yrs   \n",
       "2  retraining     841               ... <  100 DM    4 <= ... < 7 yrs   \n",
       "3      others    2122  unknown/no savings account    1 <= ... < 4 yrs   \n",
       "4      others    2171  unknown/no savings account    1 <= ... < 4 yrs   \n",
       "\n",
       "  installment_rate                   personal_status_sex other_debtors  ...  \\\n",
       "0             < 20  female : non-single or male : single          none  ...   \n",
       "1   25 <= ... < 35                male : married/widowed          none  ...   \n",
       "2   25 <= ... < 35  female : non-single or male : single          none  ...   \n",
       "3   20 <= ... < 25                male : married/widowed          none  ...   \n",
       "4             < 20                male : married/widowed          none  ...   \n",
       "\n",
       "                property age  other_installment_plans   housing  \\\n",
       "0           car or other  21                     none  for free   \n",
       "1  unknown / no property  36                     none  for free   \n",
       "2  unknown / no property  23                     none  for free   \n",
       "3  unknown / no property  39                     none  for free   \n",
       "4           car or other  38                     bank      rent   \n",
       "\n",
       "  number_credits                        job people_liable telephone  \\\n",
       "0              1  skilled employee/official        0 to 2        no   \n",
       "1          3-Feb  skilled employee/official     3 or more        no   \n",
       "2              1       unskilled - resident        0 to 2        no   \n",
       "3          3-Feb       unskilled - resident     3 or more        no   \n",
       "4          3-Feb       unskilled - resident        0 to 2        no   \n",
       "\n",
       "  foreign_worker credit_risk  \n",
       "0             no        good  \n",
       "1             no        good  \n",
       "2             no        good  \n",
       "3            yes        good  \n",
       "4            yes        good  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read the file\n",
    "df = pd.read_csv(r\"C:\\Users\\dideu\\OneDrive\\Documents\\DDB\\thesis\\Thesis_Project\\South German Credit  Running RF on the new data\\german_credit.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Random Forest model\n",
    "def run_RF (X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "        # Preprocessing for cataegorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "    \n",
    "    # Preprocessing for numerical data - StandardScaler\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())])\n",
    "    \n",
    "    ordinal_encode = Pipeline(steps=[\n",
    "        ('ordinal', OrdinalEncoder())])\n",
    "    \n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            #('ordcat', ordinal_encode, ordinal_categorical),  \n",
    "            ('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)],\n",
    "        remainder='passthrough')\n",
    "    \n",
    "    clf_RF = Pipeline(steps=[('preprocessor', transformations), ('classifier', RandomForestClassifier())])\n",
    "    model_RF = clf_RF.fit(X_fit, y_fit)\n",
    "    y_pred_rf = model_RF.predict(X_test)\n",
    "    y_pred_proba_rf = model_RF.predict_proba(X_test)\n",
    "\n",
    "    f1_rf = f1_score(y_test, y_pred_rf)\n",
    "    roc_rf = roc_auc_score(y_test, y_pred_rf)\n",
    "    auc_sec = roc_auc_score(y_test, y_pred_proba_rf[:,1])\n",
    "    a = len(X_fit)\n",
    "    # Saving the model performance metrics\n",
    "    models_rf = [('RF_{}'.format(model_name), k, j, i, a, f1_rf, roc_rf, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metrics_rf = pd.DataFrame(models_rf, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num', 'X_fit Size',  'F1 (%)', 'AUC (%)', 'F1_std(%)', 'AUC_std(%)'])\n",
    "      \n",
    "    return model_RF, model_perf_metrics_rf, roc_rf, f1_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Logistic Regression without scaling\n",
    "def run_LR(X_fit, y_fit, X_test, y_test, model_name, i, j , k):\n",
    "    # Preprocessing for categorical data - OneHotEncoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    # Preprocessing for numerical data - StandardScaler\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    ordinal_encode = Pipeline(steps=[\n",
    "        ('ordinal', OrdinalEncoder())])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "    transformations = ColumnTransformer(\n",
    "        transformers=[\n",
    "            #('ordcat', ordinal_encode, ordinal_categorical),\n",
    "            ('num', numerical_transformer, numerical),  \n",
    "            ('cat', categorical_transformer, categorical)], \n",
    "        remainder='passthrough')\n",
    "\n",
    "# Logistic Regression\n",
    "    clf_LR = Pipeline(steps=[('preprocessor', transformations), ('classifier', LogisticRegression(max_iter=1000))])\n",
    "    model_LR = clf_LR.fit(X_fit, y_fit)\n",
    "    y_pred_lr = model_LR.predict(X_test)\n",
    "\n",
    "    f1_lr = f1_score(y_test, y_pred_lr)\n",
    "    roc_lr = roc_auc_score(y_test, y_pred_lr)\n",
    "    a = len(X_fit)\n",
    "\n",
    "    # Saving the model performance metrics\n",
    "    models_lr = [('LR_{}'.format(model_name), k, j, i, a, f1_lr, roc_lr, 0, 0)]\n",
    "    # Saving the model performance metrics in a dataframe\n",
    "    model_perf_metric_lr = pd.DataFrame(models_lr, columns = ['Model', 'Iteration', 'Sample Size', 'CF Num','X_fit Size', 'F1 (%)', 'AUC (%)', 'F1_std(%)', 'AUC_std(%)'])\n",
    "\n",
    "    return  model_perf_metric_lr, roc_lr, f1_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'] = df['status'].str.replace('... >= 200 DM / salary for at least 1 year', '0')\n",
    "df['status'] = df['status'].str.replace('no checking account', '1')\n",
    "df['status'] = df['status'].str.replace('... < 0 DM', '2')\n",
    "df['status'] = df['status'].str.replace('0<= ... < 200 DM', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['installment_rate'] = df['installment_rate'].str.replace('< 20', '0')\n",
    "df['installment_rate'] = df['installment_rate'].str.replace('20 <= ... < 25', '1')\n",
    "df['installment_rate'] = df['installment_rate'].str.replace('25 <= ... < 35', '2')\n",
    "df['installment_rate'] = df['installment_rate'].str.replace('>= 35', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the target column with string '0' and '1'.\n",
    "df['employment_duration'] = df['employment_duration'].str.replace('< 1 yr', '1')  \n",
    "df['employment_duration'] = df['employment_duration'].str.replace('unemployed' , '0') \n",
    "df['employment_duration'] = df['employment_duration'].str.replace('1 <= ... < 4 yrs', '2')\n",
    "df['employment_duration'] = df['employment_duration'].str.replace('4 <= ... < 7 yrs', '3')\n",
    "df['employment_duration'] = df['employment_duration'].str.replace('>= 7 yrs', '4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['present_residence'] = df['present_residence'].str.replace('>= 7 yrs', '0')\n",
    "df['present_residence'] = df['present_residence'].str.replace('4 <= ... < 7 yrs', '1')\n",
    "df['present_residence'] = df['present_residence'].str.replace('1 <= ... < 4 yrs', '2')\n",
    "df['present_residence'] = df['present_residence'].str.replace('< 1 yr', '3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['installment_rate'] = df['installment_rate'].astype(int)\n",
    "df['employment_duration'] = df['employment_duration'].astype(int)\n",
    "df['present_residence'] = df['present_residence'].astype(int)\n",
    "df['status'] = df['status'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the values in the target column with string '0' and '1'.\n",
    "df['credit_risk'] = df['credit_risk'].str.replace('bad', '0')  \n",
    "df['credit_risk'] = df['credit_risk'].str.replace('good', '1')   \n",
    "\n",
    "# Convert to integer\n",
    "df['credit_risk'] = df['credit_risk'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From X set, education column is also dropped as it is similar to Education-num\n",
    "X = df.drop(['credit_risk'], axis=1)\n",
    "\n",
    "y = df['credit_risk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into fitting data (60%) and test set (40%)\n",
    "X_fit, X_test, y_fit,y_test = train_test_split(X, y, test_size=0.3, random_state=1155)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the columns' categories\n",
    "numerical = ['duration',  'amount', 'age', 'employment_duration', 'status','installment_rate', 'present_residence']\n",
    "categorical = ['personal_status_sex', 'property', 'other_installment_plans', 'housing' ]\n",
    "ordinal_categorical = [ 'credit_history','purpose', 'savings', 'other_debtors', 'number_credits', 'job', 'people_liable', 'telephone', 'foreign_worker']\n",
    "categorical = categorical + ordinal_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'auc_def_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m iteration_num \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m _ , model_perf_metrics_mean_of_iterations_on_X_test_SG \u001b[38;5;241m=\u001b[39m \u001b[43mmean_of_default_models\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX_test\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[77], line 24\u001b[0m, in \u001b[0;36mmean_of_default_models\u001b[1;34m(k, X_fit, y_fit, X_test, y_test, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Creating a list appending the auc and f1 scores of each iteration to calculate mean in at the end of the iterations.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m total_auc_rf\u001b[38;5;241m.\u001b[39mappend(auc_def_rf)\n\u001b[1;32m---> 24\u001b[0m total_auc_lr\u001b[38;5;241m.\u001b[39mappend(\u001b[43mauc_def_lr\u001b[49m)\n\u001b[0;32m     25\u001b[0m total_f1_rf\u001b[38;5;241m.\u001b[39mappend(f1_def_rf)\n\u001b[0;32m     26\u001b[0m total_f1_lr\u001b[38;5;241m.\u001b[39mappend(f1_def_lr)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'auc_def_lr' is not defined"
     ]
    }
   ],
   "source": [
    "iteration_num = 2\n",
    "# Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "_ , model_perf_metrics_mean_of_iterations_on_X_test_SG = mean_of_default_models (iteration_num, X_fit, y_fit, X_test, y_test, 'X_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the default RF and LR models for multiple iterations to calculate the mean and standard deviation of the performance metrics.\n",
    "_, model_perf_metrics_mean_of_iterations_on_X_fit_SG = mean_of_default_models (iteration_num, X_fit, y_fit, X_fit, y_fit, 'X_fit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Sample Size</th>\n",
       "      <th>CF Num</th>\n",
       "      <th>X_fit Size</th>\n",
       "      <th>F1 (%)</th>\n",
       "      <th>F1_std(%)</th>\n",
       "      <th>AUC (%)</th>\n",
       "      <th>AUC_std(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF_default_mean_of_iterations tested on: X_test</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.657295</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.764444</td>\n",
       "      <td>0.001332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR_default_mean_of_iterations tested on: X_test</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.382651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.613622</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RF_default_mean_of_iterations tested on: X_fit</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.999337</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.999467</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LR_default_mean_of_iterations tested on: X_fit</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27133</td>\n",
       "      <td>0.381326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.612888</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Model  Iteration  Sample Size  \\\n",
       "0  RF_default_mean_of_iterations tested on: X_test         30            0   \n",
       "1  LR_default_mean_of_iterations tested on: X_test         30            0   \n",
       "2   RF_default_mean_of_iterations tested on: X_fit         30            0   \n",
       "3   LR_default_mean_of_iterations tested on: X_fit         30            0   \n",
       "\n",
       "   CF Num  X_fit Size    F1 (%)  F1_std(%)   AUC (%)  AUC_std(%)  \n",
       "0       0       27133  0.657295   0.001947  0.764444    0.001332  \n",
       "1       0       27133  0.382651   0.000000  0.613622    0.000000  \n",
       "2       0       27133  0.999337   0.000065  0.999467    0.000093  \n",
       "3       0       27133  0.381326   0.000000  0.612888    0.000000  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perf_metrics_merged_SG = model_perf_metrics_mean_of_iterations_on_X_test.merge(model_perf_metrics_mean_of_iterations_on_X_fit, how='outer')\n",
    "model_perf_metrics_merged_SG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>Model</th>\n",
    "      <th>Iteration</th>\n",
    "      <th>Sample Size</th>\n",
    "      <th>CF Num</th>\n",
    "      <th>X_fit Size</th>\n",
    "      <th>F1 (%)</th>\n",
    "      <th>F1_std(%)</th>\n",
    "      <th>AUC (%)</th>\n",
    "      <th>AUC_std(%)</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>RF_default_mean_of_iterations tested on: X_test</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.657382</td>\n",
    "      <td>0.002113</td>\n",
    "      <td>0.764474</td>\n",
    "      <td>0.001373</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>LR_default_mean_of_iterations tested on: X_test</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.647489</td>\n",
    "      <td>0.000000</td>\n",
    "      <td>0.755191</td>\n",
    "      <td>0.000000</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>RF_default_mean_of_iterations tested on: X_fit</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.999322</td>\n",
    "      <td>0.000085</td>\n",
    "      <td>0.999427</td>\n",
    "      <td>0.000110</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>LR_default_mean_of_iterations tested on: X_fit</td>\n",
    "      <td>30</td>\n",
    "      <td>0</td>\n",
    "      <td>0</td>\n",
    "      <td>27133</td>\n",
    "      <td>0.641738</td>\n",
    "      <td>0.000000</td>\n",
    "      <td>0.752036</td>\n",
    "      <td>0.000000</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "banana",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
